{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate efficacy of gain-correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from preprocVisualisationTesting.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Load a trained model\n",
    "import torch\n",
    "import math\n",
    "import gpytorch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "import preprocUtils\n",
    "import preprocRandomVariables\n",
    "import preprocLikelihoods\n",
    "import preprocModels\n",
    "import preprocKernels\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Plotly \n",
    "import plotly\n",
    "from plotly.offline import iplot as plt\n",
    "from plotly import graph_objs as plt_type\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import colorcet # For custom colormaps\n",
    "\n",
    "import nbimporter\n",
    "from preprocVisualisationTesting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(data_id = '0', prior='noPrior', lik='linLik', stamp = '_00_firstRun_noPCremoved',\n",
    "                    data_dir='/nfs/data/gergo/Neurofinder_update/', device = 'cpu', \n",
    "                    retVars=False, retResults=False, subdataset = '00'):\n",
    "\n",
    "    dataset_name = 'neurofinder.0' + data_id +'.' + subdataset\n",
    "    mll = torch.load(data_dir + dataset_name +'/preproc2P/savedModels/mll_' +prior+'_' + lik + stamp, map_location=device)\n",
    "\n",
    "    model = mll.model\n",
    "    likelihood = mll.likelihood\n",
    "    mean_im = mll.mean_im\n",
    "\n",
    "    train_x = mll.train_x\n",
    "    train_y = mll.train_y\n",
    "\n",
    "    dataStats = preprocUtils.getDataStatistics(train_x, train_y)\n",
    "    \n",
    "    print(dataStats)\n",
    "    print(OrderedDict(likelihood.named_parameters()))\n",
    "    \n",
    "    \n",
    "    # Set the model and likelihood in evaluation mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    # Create test grids over which we predict for easy visualisations\n",
    "    n_test_grid = torch.tensor(mean_im.shape)\n",
    "    n_test_grid_small = 32\n",
    "    test_x = preprocUtils.create_test_grid(n_test_grid, ndims=2, device=device, a=dataStats['x_minmax'][0,:], b=dataStats['x_minmax'][1,:])\n",
    "    test_x_small = preprocUtils.create_test_grid(n_test_grid_small, ndims=2, device=device, a=dataStats['x_span'][0][0], b=dataStats['x_span'][0][1])\n",
    "    \n",
    "    # Get log_photon counts:\n",
    "    pred_log_photon = model(test_x)\n",
    "    if isinstance(model.mean_module, gpytorch.means.ConstantMean):\n",
    "        pred_gain_func = (pred_log_photon.mean()-model.mean_module.constant.data).exp()\n",
    "    else:\n",
    "        pred_gain_func = pred_log_photon.mean().exp()\n",
    "    \n",
    "    divBy = pred_gain_func.reshape(*mean_im.shape)\n",
    "    gainRange = [0.1, 10.]\n",
    "\n",
    "    corr_mean_im = (mean_im).div(torch.clamp(divBy, min=1./gainRange[1], max=1./gainRange[0]))\n",
    "    #corr_mean_im = (mean_im-likelihood.offset).div(torch.clamp(divBy, min=1./gainRange[1], max=1./gainRange[0]))+likelihood.offset\n",
    "    \n",
    "    if not retVars:\n",
    "        imagesc(pred_gain_func.reshape(*n_test_grid), \n",
    "        title = 'Expected gain function',\n",
    "           heatmap=dict(colorscale = 'div'))\n",
    "    \n",
    "        # Show the original and the gain_corrected images\n",
    "        imagesc(mean_im, title = 'Mean image')\n",
    "        imagesc(corr_mean_im, title = 'Corrected mean image')\n",
    "\n",
    "    if retVars:\n",
    "        return mll, model, likelihood, train_x, train_y, dataStats, mean_im, pred_gain_func.reshape(*n_test_grid), corr_mean_im\n",
    "    \n",
    "    if retResults:\n",
    "        return mean_im, pred_gain_func.reshape(*n_test_grid), corr_mean_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_git = '_gitsha_' + '2bd0d720de0995be6b0f1795304839f9877cb6c3'\n",
    "stamp_training_type = '_rPC_1_origPMgain_useNans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-241a67341eca>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-241a67341eca>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stamp_trainingCoverage = '_targetCoverage_05'\n",
    "#stamp_modelGridType = '_grid_30_7'\n",
    "stamp_modelGridType = '_grid_30_7_largeBatch'\n",
    "data_id = '0'\n",
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = data_id,  \n",
    "                prior='noPrior', \n",
    "                lik='unampLik', \n",
    "                stamp = stamp_git + stamp_training_type + stamp_trainingCoverage + stamp_modelGridType\n",
    "               )\n",
    "\n",
    "imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "imagesc(mean_im, pixels_per_micron=1.15)\n",
    "imagesc(corr_mean_im, pixels_per_micron=1.15)\n",
    "#imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = '0'\n",
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = data_id,  prior='noPrior', lik='linLik', \n",
    "                stamp = stamp_git + stamp_training_type)\n",
    "\n",
    "imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "imagesc(mean_im, pixels_per_micron=1.15)\n",
    "imagesc(corr_mean_im, pixels_per_micron=1.15)\n",
    "#imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_id = '0'\n",
    "# mll, model, likelihood, train_x, train_y, \\\n",
    "# dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "# display_results(retVars=True,data_id = data_id,  prior='noPrior', lik='unampLik', stamp = '_00_firstRun_noPCremoved')\n",
    "\n",
    "# imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "# imagesc(mean_im, pixels_per_micron=1.15)\n",
    "# imagesc(corr_mean_im, pixels_per_micron=1.15)\n",
    "# #imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_id = '0'\n",
    "# subdataset = '00'\n",
    "# mll, model, likelihood, train_x, train_y, \\\n",
    "# dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "# display_results(retVars=True,data_id = data_id, prior='noPrior', lik='poissLik', stamp = '_05_origPMGain_test_03',\n",
    "#                subdataset = subdataset)\n",
    "\n",
    "# imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "# imagesc(mean_im, pixels_per_micron=1.15)\n",
    "# imagesc(corr_mean_im, pixels_per_micron=1.15)\n",
    "# #imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_id = '4'\n",
    "# subdataset = '00'\n",
    "# mll, model, likelihood, train_x, train_y, \\\n",
    "# dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "# display_results(retVars=True,data_id = data_id, prior='expertPrior', lik='poissLik', stamp = '_05_origPMGain_test_05_finegrid_50_3interp',\n",
    "#                subdataset = subdataset)\n",
    "\n",
    "# imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "# imagesc(mean_im, pixels_per_micron=1.15)\n",
    "# imagesc(corr_mean_im, pixels_per_micron=1.15)\n",
    "# #imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_id = '1'\n",
    "# subdataset = '00'\n",
    "# mll, model, likelihood, train_x, train_y, \\\n",
    "# dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "# display_results(retVars=True,data_id = data_id, prior='expertPrior', lik='linLik', stamp = '_05_origPMGain_test_04',\n",
    "#                subdataset = subdataset)\n",
    "\n",
    "# imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "# imagesc(mean_im, pixels_per_micron=1.15)\n",
    "# imagesc(corr_mean_im, pixels_per_micron=1.15)\n",
    "# #imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_y[:,0] <= torch.max(likelihood.offset, torch.tensor(0.)).data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = '0'\n",
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = data_id, prior='expertPrior', lik='unampLik', stamp = '_02_origPMGain_test_05')\n",
    "\n",
    "imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "imagesc(mean_im, pixels_per_micron=1.15)\n",
    "imagesc(corr_mean_im.clamp(max=1000), pixels_per_micron=1.15)\n",
    "#imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mll.pmGain_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagesc(mll.pmGain_y.reshape(mean_im.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "# imagesc(mean_im, pixels_per_micron=1.15)\n",
    "# imagesc(corr_mean_im, pixels_per_micron=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_id = '0'\n",
    "# mll, model, likelihood, train_x, train_y, \\\n",
    "# dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "# display_results(retVars=True,data_id = data_id, prior='noPrior', lik='poissLik', stamp = '_02_origPMGain_test_05')\n",
    "\n",
    "# # imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "# # imagesc(mean_im, pixels_per_micron=1.15)\n",
    "# # imagesc(corr_mean_im.clamp(max=20), pixels_per_micron=1.15)\n",
    "# #imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_id = '0'\n",
    "# mll, model, likelihood, train_x, train_y, \\\n",
    "# dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "# display_results(retVars=True,data_id = data_id, prior='noPrior', lik='poissLik', stamp = '_02_normPMGain_test_05')\n",
    "\n",
    "# # imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "# # imagesc(mean_im, pixels_per_micron=1.15)\n",
    "# # imagesc(corr_mean_im.clamp(max=20), pixels_per_micron=1.15)\n",
    "# #imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = '0'\n",
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = data_id, prior='expertPrior', lik='linLik', stamp = '_05_origPMGain_test_05_finegrid_50_3interp')\n",
    "\n",
    "imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "imagesc(mean_im, pixels_per_micron=1.15)\n",
    "imagesc(corr_mean_im, pixels_per_micron=1.15)\n",
    "#imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = '0'\n",
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = data_id, prior='noPrior', lik='linLik', stamp = '_05_origPMGain_test_05_finegrid_50_3interp')\n",
    "\n",
    "imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "imagesc(mean_im, pixels_per_micron=1.15)\n",
    "imagesc(corr_mean_im, pixels_per_micron=1.15)\n",
    "#imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(corr_mean_im, pixels_per_micron=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def logistic(x,  x0=0., k=1., L=1.):\n",
    "#     return x.add(-x0).mul(-k).exp().add(1).reciprocal().mul(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic(likelihood.logit_underamplified_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_id = '1'\n",
    "# mll, model, likelihood, train_x, train_y, \\\n",
    "# dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "# display_results(retVars=True,data_id = data_id, prior='noPrior', lik='poissLik', stamp = '_02_normPMGain_test_05')\n",
    "\n",
    "# # imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "# # imagesc(mean_im, pixels_per_micron=1.15)\n",
    "# # imagesc(corr_mean_im.clamp(max=20), pixels_per_micron=1.15)\n",
    "# #imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_priors(module):\n",
    "    for name, param, prior in module.named_parameter_priors():\n",
    "        try:\n",
    "            print(name, float(param), [float(prior.a), float(prior.b)])#, prior.log_prob(param)\n",
    "        except:\n",
    "            print(name, prior.log_prob(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_priors(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict(mll.likelihood.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy import array, zeros\n",
    "from scipy.misc import imread\n",
    "from glob import glob\n",
    "\n",
    "# File system management\n",
    "import os\n",
    "import errno\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "\n",
    "from preprocUtils import toTorchParam\n",
    "import copy\n",
    "\n",
    "# Evaluation metrics\n",
    "data_dir='/nfs/data/gergo/Neurofinder_update/'\n",
    "device = 'cpu'\n",
    "max_T = 1000\n",
    "\n",
    "dataset_name = 'neurofinder.0' + data_id +'.00'\n",
    "#dataset_name = 'neurofinder.0' + data_id +'.00.test'\n",
    "#dataset_name = 'neurofinder.0' + data_id +'.01.test'\n",
    "use_validation_data_only = True\n",
    "\n",
    "files = sorted(glob(data_dir+dataset_name+'/images/*.tiff'))\n",
    "imgs = np.array([imread(f) for f in files[:min(max_T, len(files))]])\n",
    "\n",
    "imgs = torch.tensor(imgs.astype(np.float32)).permute(1,2,0).to(device)\n",
    "\n",
    "savedImgsImputed = sorted(glob(data_dir+dataset_name+'/preproc2P/imgsImputed*.npy'))\n",
    "        \n",
    "if savedImgsImputed:\n",
    "    imgsImputed = torch.tensor(np.load(savedImgsImputed[-1]))\n",
    "    imgsImputedLoaded = True\n",
    "\n",
    "    if imgsImputed.size(2) < max_T:\n",
    "        warnings.warn(\"\"\"In the saved imgsImputed data there is only {} frames,\n",
    "                      less than the requested {}, using only available number\"\"\"\n",
    "                      .format(imgsImputed.size(2), max_T))\n",
    "    else:\n",
    "        imgsImputed = imgsImputed[:,:,:max_T]\n",
    "else:\n",
    "    imgsImputed = imgs\n",
    "\n",
    "        \n",
    "if use_validation_data_only:\n",
    "    imgs = imgs[:,:,500:]\n",
    "    imgsImputed = imgsImputed[:,:,500:]\n",
    "        \n",
    "# Correct for pmGain, if mll has it\n",
    "\n",
    "if hasattr(mll, 'pmGain_y'):\n",
    "    imgsImputed.div_(mll.pmGain_y.reshape(*imgsImputed.shape[:2]).unsqueeze(-1))\n",
    "    \n",
    "imgsImputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLambdaLogProb(log_w, log_lam, dim=-1, requires_grad=True):\n",
    "    if dim == -1:\n",
    "        dim=log_w.ndimension()-1\n",
    "    # Get the log probabilities (numerically stable), then logsumexp()\n",
    "    x = torch.arange(log_w.size(dim), device=log_w.device).float().view(*([1]*max(dim,0))+[-1]+[1]*max(log_w.ndimension()-dim-1,0))\n",
    "\n",
    "    lprobs = (log_w\n",
    "              - (x+1.).lgamma()\n",
    "              + x*log_lam\n",
    "              -log_lam.exp())\n",
    "\n",
    "    del x\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if requires_grad:\n",
    "        return lprobs.logsumexp(dim=dim)\n",
    "    else:\n",
    "        return lprobs.logsumexp(dim=dim).data\n",
    "    \n",
    "# Define an optimiser that starts from MAP estimate and uses lambda log prob as objective\n",
    "class LambdaOptimiser(torch.nn.Module):\n",
    "    def __init__(self, log_w, lambda_guess=torch.tensor(1.)):\n",
    "        super(LambdaOptimiser, self).__init__()\n",
    "        self.register_buffer(\"log_w\", log_w)\n",
    "        \n",
    "        # Put log lambda into the appropriate shape\n",
    "        log_lam = lambda_guess.float().clamp(min=1e-1).log()\n",
    "        log_lam = log_lam.view(list(log_lam.size())+[1]*max(0, log_w.ndimension()-log_lam.ndimension()))\n",
    "        self.register_parameter(\"log_lam\", preprocUtils.toTorchParam(log_lam, \n",
    "                                                                     paramShape=log_lam.size(), \n",
    "                                                                     device=log_lam.device))\n",
    "        \n",
    "    def forward(self):\n",
    "        return -getLambdaLogProb(self.log_w, self.log_lam).sum()\n",
    "        \n",
    "\n",
    "def getOptLambda(log_w, lambda_guess=None): # Opt lambda given discrete estimate of photon distribution\n",
    "#     if log_w[0]==0: # Special case of certainty\n",
    "#         return torch.tensor([0.])\n",
    "    lambda_guess = lambda_guess if lambda_guess is not None else torch.tensor(1., device=log_w.device)\n",
    "    lamModule = LambdaOptimiser(log_w.detach(), lambda_guess=lambda_guess)\n",
    "    optim = torch.optim.LBFGS(lamModule.parameters()) \n",
    "    # LBFGS does optimisation internally via \"closure\", no need to iterate outside\n",
    "    def closure():\n",
    "        optim.zero_grad()\n",
    "        loss = lamModule()\n",
    "        #print(loss, lamModule.log_lam.exp().data)\n",
    "        loss.backward()\n",
    "        return loss  \n",
    "\n",
    "    optim.step(closure)\n",
    "    \n",
    "    out = lamModule.log_lam.exp().data\n",
    "    \n",
    "    del lamModule\n",
    "    \n",
    "    return out\n",
    "    \n",
    "\n",
    "    \n",
    "# class LambdaLikOptimiser(torch.nn.Module):\n",
    "#     def __init__(self, likelihood, lambda_guess=torch.tensor([1.]), max_photon = 50.):\n",
    "#         super(LambdaLikOptimiser, self).__init__()\n",
    "#         self.likelihood = likelihood\n",
    "#         self.register_parameter(\"log_lam\", preprocUtils.toTorchParam(lambda_guess.float().clamp(min=1e-1).log(), ndims=0))\n",
    "        \n",
    "#     def forward(self, cur_target):\n",
    "#         return -self.likelihood.single_log_prob( \n",
    "#                                     self.log_lam.expand(cur_target.size()).view(1,-1), \n",
    "#                                     cur_target, batchsize = int(450), max_photon= max_photon).sum()\n",
    "    \n",
    "# def getOptLambdaFromLik(data, likelihood, lambda_guess=torch.tensor([1.]), max_photon = 50.):\n",
    "#     lamModule = LambdaLikOptimiser(likelihood, lambda_guess=lambda_guess, max_photon = max_photon)\n",
    "#     optim = torch.optim.LBFGS([lamModule.log_lam])\n",
    "#     def closure():\n",
    "#         optim.zero_grad()\n",
    "#         loss = lamModule(data)\n",
    "#         #print(loss, lamModule.log_lam.exp().data)\n",
    "#         loss.backward()\n",
    "#         return loss  \n",
    "#     optim.step(closure)\n",
    "        \n",
    "#     return lamModule.log_lam.exp().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2logPhotonProb(im, photon_log_probs, gray_levels, interpolate=False):\n",
    "    if not interpolate:\n",
    "        # Nearest neighbor version\n",
    "        return photon_log_probs[\n",
    "            (im.unsqueeze(-1) - gray_levels.view(*([1]*im.ndimension()+[-1]))).abs().min(-1)[1],:]\n",
    "    else:\n",
    "\n",
    "        # Interpolation version\n",
    "        dists = (im.unsqueeze(-1) - gray_levels.view(*([1]*im.ndimension()+[-1]))) # Distances along last dimension\n",
    "\n",
    "        # Find the last element in gray_levels that im is larger than (so dists>=0), then interpolate or extrapolate appropriately\n",
    "        last_pos_ind = (((dists>=0).sum(-1))-1).clamp(0, dists.size(-1)-2).unsqueeze(-1)\n",
    "        interp_inds = torch.cat([last_pos_ind, last_pos_ind+1], dim=-1)\n",
    "\n",
    "        static_indices = np.indices(interp_inds.shape)\n",
    "        static_indices[-1] = interp_inds\n",
    "        dists_sorted = dists[static_indices]\n",
    "\n",
    "\n",
    "        # Get the two closest distances, and values to linearly inter-/extra-polate\n",
    "        dist0 = dists_sorted[...,0]\n",
    "        dist1 = dists_sorted[...,1]\n",
    "\n",
    "        val0 = photon_log_probs[interp_inds[...,0],:]\n",
    "        val1 = photon_log_probs[interp_inds[...,1],:]\n",
    "\n",
    "        dist_sign_same = ((dist0*dist1)>=0.).float()\n",
    "\n",
    "        denom = (\n",
    "            # If different sign (interpolate)\n",
    "            (1.-dist_sign_same)*(dist1.abs()+dist0.abs()) \n",
    "            # If same sign (extrapolate)\n",
    "            + dist_sign_same *((dist0-dist1).abs())\n",
    "        )\n",
    "\n",
    "\n",
    "        rx = dist0.unsqueeze(-1)\n",
    "\n",
    "        m = (val1-val0)/denom.unsqueeze(-1)\n",
    "\n",
    "\n",
    "        fx = m*rx + val0\n",
    "\n",
    "\n",
    "        return fx\n",
    "\n",
    "# imgsImputedCorr = preprocUtils.apply(lambda x: correctImage(x, inverse_poiss_mean, gray_levels), \n",
    "#                                      copy.deepcopy(imgsImputed), dim=2)\n",
    "\n",
    "def im2photon(im, inverse_poiss_MAP, gray_levels, keep_zeros=True):\n",
    "    # Linear inter/extra-polation version\n",
    "    \n",
    "    dists = (im.unsqueeze(-1) - gray_levels.view(*([1]*im.ndimension()+[-1]))) # Distances along last dimension\n",
    "\n",
    "    # Find the last element in gray_levels that im is larger than (so dists>=0), then interpolate or extrapolate appropriately\n",
    "    last_pos_ind = (((dists>=0).sum(-1))-1).clamp(0, dists.size(-1)-2).unsqueeze(-1)\n",
    "    interp_inds = torch.cat([last_pos_ind, last_pos_ind+1], dim=-1)\n",
    "\n",
    "    static_indices = np.indices(interp_inds.shape)\n",
    "    static_indices[-1] = interp_inds\n",
    "    dists_sorted = dists[static_indices]\n",
    "\n",
    "\n",
    "    # Get the two closest distances, and values to linearly inter-/extra-polate\n",
    "    dist0 = dists_sorted[...,0]\n",
    "    dist1 = dists_sorted[...,1]\n",
    "\n",
    "    val0 = inverse_poiss_MAP[interp_inds[...,0]]\n",
    "    val1 = inverse_poiss_MAP[interp_inds[...,1]]\n",
    "\n",
    "    dist_sign_same = ((dist0*dist1)>=0.).float()\n",
    "\n",
    "    denom = (\n",
    "        # If different sign (interpolate)\n",
    "        (1.-dist_sign_same)*(dist1.abs()+dist0.abs()) \n",
    "        # If same sign (extrapolate)\n",
    "        + dist_sign_same *((dist0-dist1).abs())\n",
    "    )\n",
    "\n",
    "\n",
    "    rx = dist0\n",
    "\n",
    "    m = (val1-val0)/denom\n",
    "\n",
    "\n",
    "    fx = m*rx + val0\n",
    "\n",
    "    if keep_zeros:\n",
    "        fx = fx * (im!=0).type(fx.type())\n",
    "    \n",
    "    return fx\n",
    "    \n",
    "    \n",
    "    # Nearest neighbor version\n",
    "    #return inverse_poiss_MAP[(im.unsqueeze(-1) - gray_levels.view(*([1]*im.ndimension()+[-1]))).abs().min(-1)[1]]\n",
    "\n",
    "# imgsImputedCorr = preprocUtils.apply(lambda x: correctImage(x, inverse_poiss_mean, gray_levels), \n",
    "#                                      copy.deepcopy(imgsImputed), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating photon probability from grey level\n",
    "\n",
    "light_levels = torch.arange(1e-4, 1.,1e-2) # In photon\n",
    "model_out = gpytorch.random_variables.GaussianRandomVariable(light_levels.log(), gpytorch.lazy.DiagLazyVariable(1e-7*torch.ones_like(light_levels)))\n",
    "\n",
    "\n",
    "max_photon = float(70)\n",
    "photon_counts, photon_log_probs = (\n",
    "    likelihood.getPhotonLogProbs(model_out.mean().view(-1,1).exp(), max_photon=max_photon, reNormalise = False))\n",
    "p_PM = likelihood.createResponseDistributions(photon_counts)\n",
    "\n",
    "gray_levels = torch.cat([torch.tensor([-0.02*np.nanmax(train_y), 0., 1e-10]), # Important to add a non-zero value close to zero\n",
    "                         torch.logspace(-3, imgsImputed.max().log10(),100)])\n",
    "#                           torch.linspace(0., 3., 100),\n",
    "#                          torch.linspace(3.1, imgsImputed.max(), 5)])\n",
    "\n",
    "\n",
    "photon_log_probs = likelihood.getLogProbSumOverTargetSamples(p_PM, gray_levels.view(-1))\n",
    "\n",
    "# Fix the 0 gray_level issue (by making it certainly 0 photon)\n",
    "# photon_log_probs[:2, 0] = 0.\n",
    "# photon_log_probs[:2, 1:] = -float('inf')\n",
    "\n",
    "#log_photon_prob_marginals = photon_log_probs.exp().div(photon_log_probs.exp().sum(1).view(-1,1)).log()\n",
    "log_photon_prob_marginals = photon_log_probs - photon_log_probs.logsumexp(1).unsqueeze(1)\n",
    "\n",
    "plot(log_photon_prob_marginals[:,:10].exp(), gray_levels)\n",
    "\n",
    "#print(gray_levels[:50])\n",
    "#plot(photon_prob_marginals[:50,:].t())\n",
    "\n",
    "\n",
    "\n",
    "tmp = log_photon_prob_marginals.detach().exp().data\n",
    "n_max = 7\n",
    "fig = plotStacked(torch.cat([tmp[:,:n_max], (1.-tmp[:,:n_max].sum(1)).view(-1,1)],dim=1), gray_levels, now=False)\n",
    "plt(fig)\n",
    "\n",
    "# Plot gamma posterior mean and variance (single sample, weigthed sum of p(num_photon)*num_photon)\n",
    "\n",
    "\n",
    "#inverse_poiss_mean = (tmp_marginals*torch.arange(max_photon).view(1,-1)).sum(1)\n",
    "\n",
    "# inverse_poiss_alpha = (tmp_marginals*torch.arange(max_photon).view(1,-1)).sum(1)\n",
    "# inverse_poiss_beta = torch.arange(max_photon).view(1,-1).sum(1)/max_photon\n",
    "\n",
    "#inverse_poiss_MAP = tmp_marginals.max(1)[1]\n",
    "inverse_poiss_MAP = torch.cat([getOptLambda(log_photon_prob_marginals[i,:], \n",
    "                                            lambda_guess=log_photon_prob_marginals[i,:].max(0)[1]) \n",
    "                               for i in range(log_photon_prob_marginals.size(0))])\n",
    "\n",
    "inverse_poiss_MAP2 = torch.cat([getOptLambda(photon_log_probs[i,:], \n",
    "                                            lambda_guess=photon_log_probs[i,:].max(0)[1]) \n",
    "                               for i in range(photon_log_probs.size(0))])\n",
    "\n",
    "#inverse_poiss_MAP = torch.cat([getOptLambdaFromLik(gray_levels[i].view(-1), likelihood) for i in range(log_photon_prob_marginals.size(0))])\n",
    "\n",
    "#plot(inverse_poiss_MAP.view(-1,1), gray_levels)\n",
    "\n",
    "plt(plot(inverse_poiss_MAP.view(-1,1), gray_levels, now=False)\n",
    "    +plot(inverse_poiss_MAP2.view(-1,1), gray_levels, now=False)\n",
    "   )\n",
    "\n",
    "\n",
    "# plot(torch.cat([inverse_poiss_MAP.view(-1,1), \n",
    "#                 im2photon(torch.arange(0,gray_levels.max()*1.1,1e-1), inverse_poiss_MAP, gray_levels).view(-1,1)], dim=1), \n",
    "#                gray_levels)\n",
    "\n",
    "#imgsImputedCorr = copy.deepcopy(imgsImputedCorr)\n",
    "\n",
    "\n",
    "\n",
    "# exportFigure(fig, image='svg', filename='fig1-photon_cum_prob')\n",
    "\n",
    "# plt(plt_type.Figure(data=data, \n",
    "#                 layout=plt_type.Layout(\n",
    "#                     xaxis=dict(\n",
    "#                         title='Grey level in data'\n",
    "#                     ),\n",
    "#                     yaxis=dict(\n",
    "#                         title='Cumulative probability of photon count'\n",
    "#                     )\n",
    "#                 ))\n",
    "#    )\n",
    "\n",
    "# likelihood.__class__\n",
    "\n",
    "# tmp = likelihood.single_log_prob(torch.arange(0.,20.).view(1,-1)*torch.ones_like(gray_levels).view(-1,1), gray_levels.view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(imgsImputed.permute(2,0,1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(im2photon(imgsImputed.permute(2,0,1)[0], inverse_poiss_MAP, gray_levels, keep_zeros=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsImputedPhoton = torch.stack([im2photon(image, inverse_poiss_MAP, gray_levels, keep_zeros=True) for image in imgsImputed.permute(2,0,1)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(imgsImputedPhoton.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gainRange = [1e-20, float('inf')]\n",
    "imgsImputedCorr = ((imgsImputedPhoton)\n",
    "                   .div(torch.clamp(pred_gain_func, min=1./gainRange[1], max=1./gainRange[0]).unsqueeze(0))\n",
    "                  )\n",
    "\n",
    "#Installing packages with pip or conda within running jupyter kernel\n",
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} tifffile\n",
    "#!{sys.executable} -m pip install tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(imgsImputedCorr.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imsave\n",
    "\n",
    "for index, image in enumerate(imgsImputedCorr):\n",
    "    imsave(\n",
    "        data_dir+dataset_name+'/preproc2P/images/image' + str(index).zfill(5) + '.tif',\n",
    "        image.detach().numpy().astype('uint16')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT does this do???\n",
    "#imagesc(im2logPhotonProb(imgsImputed.permute(2,0,1)[0], photon_log_probs, gray_levels, interpolate=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgsImputedPhoton = torch.cat([im2photon(im_batch[0].permute(1,2,0), inverse_poiss_MAP, gray_levels) for im_batch in data_loader], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#orig_dataset = TensorDataset(imgsImputed.cuda())\n",
    "orig_dataset = TensorDataset(imgsImputed)\n",
    "data_loader = DataLoader(orig_dataset, batch_size=50, shuffle=False, drop_last=False)\n",
    "\n",
    "out = []\n",
    "for im_batch in data_loader:\n",
    "    #log_w = im2logPhotonProb(im_batch[0], photon_log_probs.cuda(), gray_levels.cuda())\n",
    "    log_w = im2logPhotonProb(im_batch[0], photon_log_probs, gray_levels)\n",
    "    out.append(getOptLambda(\n",
    "        log_w,\n",
    "        lambda_guess = log_w.max(-1)[1].float().mean(-1)\n",
    "    ))\n",
    "    \n",
    "imgsImputedLambda = torch.cat(out, dim=0).squeeze()\n",
    "\n",
    "\n",
    "# # Permute things so it works for DataLoader (that splits along first dimension)\n",
    "# orig_dataset = TensorDataset(imgsImputed.permute(2,0,1))\n",
    "# data_loader = DataLoader(orig_dataset, batch_size=50, shuffle=False, drop_last=False)\n",
    "\n",
    "# imgsImputedPhoton = torch.cat([im2photon(im_batch[0].permute(1,2,0), inverse_poiss_MAP, gray_levels) for im_batch in data_loader], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the imputed images for CHOMP to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_loader, orig_dataset, log_w, out\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "imgsImputedLambda = imgsImputedLambda.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(imgsImputedLambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(imgsImputedLambda.cpu()/pred_gain_func.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(pred_gain_func, heatmap={'colorscale':'div'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = copy.deepcopy(mean_im)\n",
    "tmp[train_x.long().unbind(1)] = float('nan')\n",
    "imagesc(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get goodness of fit for the model\n",
    "\n",
    "Assuming a pixel has a non-variable lambda over time, and so the noise is only due to Poissonity and photomultiplier properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_digitise(A, r0, r1, nbins=None, binsize=None  ):\n",
    "    \"\"\"Inspired by https://stackoverflow.com/questions/26783719/efficiently-get-indices-of-histogram-bins-in-python\n",
    "    \n",
    "    Treat <r0+jitter and r1<= as two seperate bins (so add 1., then clamp at 0)\n",
    "    \n",
    "    # This approach works because pytorch seem to remember sign very well even with little jitter \n",
    "    (so if A==r0, A-(r0+jitter) is negative), but (r1-(r0+jitter) / (r1-r0)) = 1, which is weird but useful\n",
    "    \"\"\"\n",
    "    \n",
    "    if binsize is None:\n",
    "        binsize=1.\n",
    "    \n",
    "    if nbins is None:\n",
    "        nbins = int(torch.tensor((r1-r0)/binsize).floor().add(2.))\n",
    "    \n",
    "    jitter = 1e-12\n",
    "    bin_center_correction = (r1-r0)/(2.*float(nbins-2.)) # So that r0 and r1 are bin centers rather than edges\n",
    "#     r0 -= bin_edge_correction\n",
    "#     r1 += bin_edge_correction\n",
    "    \n",
    "    \n",
    "    out_hists = ((A-(r0+jitter)) * (float(nbins-2)/(r1-r0))).floor().long().add(1.).clamp(0, nbins-1)\n",
    "    bin_centers = torch.cat([torch.tensor(r0).view(-1), \n",
    "                                torch.linspace(r0+bin_center_correction, r1-bin_center_correction, nbins-2) , \n",
    "                                torch.tensor(r1).view(-1)]).to(A.device)\n",
    "\n",
    "    return out_hists, bin_centers\n",
    "    \n",
    "def fast_histograms(A, r0, r1, nbins, dim = -1, output_device = None):\n",
    "    dim = dim if dim>0 else (A.ndimension()+dim)\n",
    "    output_device = output_device if output_device is not None else A.device\n",
    "\n",
    "    Adigitised, bin_centers = fast_digitise(A, r0, r1, nbins)\n",
    "    Adims = list(A.size())\n",
    "    Adims[dim]=int(bin_centers.numel())\n",
    "    Ahists = torch.zeros(*Adims, device=A.device)\n",
    "    \n",
    "    \n",
    "    for i in range(Ahists.size(dim)):\n",
    "        ind_range = [slice(None)]*max(dim,0)+[i]+[slice(None)]*max(A.ndimension()-dim-1,0)\n",
    "        Ahists[ind_range].add_((Adigitised==i).sum(dim).float())\n",
    "        \n",
    "    return Ahists.to(output_device), bin_centers.to(output_device)\n",
    "\n",
    "def fast_predictive_probs(Alambda, bin_centers, photon_log_probs, gray_levels, dim = -1, batch_size=5,\n",
    "                          interpolate=False,\n",
    "                          output_device = None):\n",
    "    \"\"\" Computes predictive probabilities of the likelihood with rates Alambda for histogram bins given\"\"\"\n",
    "    dim = dim if dim>0 else (Alambda.ndimension()+1+dim)\n",
    "    #bin_centers = torch.linspace(r0, r1, nbins, device=Alambda.device)\n",
    "    \n",
    "    # The two edges of bin_centers are assumed to be cutoff/saturation points and CDF should be computed instead of pdf\n",
    "    # At 0 this is done correctly in log_photon_prob_marginals, but not at saturation    \n",
    "    \n",
    "    log_w = im2logPhotonProb(bin_centers, photon_log_probs, gray_levels, interpolate=interpolate)\n",
    "    \n",
    "    cur_dataset = TensorDataset(Alambda.contiguous().view(-1))\n",
    "    data_loader = DataLoader(cur_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    output_device = output_device if output_device is not None else Alambda.device\n",
    "    \n",
    "    A_logpred_probs = []\n",
    "    for Alambda_batch in data_loader:\n",
    "        tmp = getLambdaLogProb(\n",
    "                log_w.unsqueeze(1),\n",
    "                Alambda_batch[0].log().view(1,-1,1), \n",
    "                requires_grad = False\n",
    "                ).to(output_device)\n",
    "        A_logpred_probs.append(tmp)\n",
    "        del tmp\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    A_logpred_probs = torch.cat(A_logpred_probs, dim=1)\n",
    "    \n",
    "    # Get normalised probabilities per bin \n",
    "    #(effectively turning log probs into a discrete distribution, \n",
    "     # as a stepwise constant approximation to the true pdf)\n",
    "    A_logpred_probs -= A_logpred_probs.logsumexp(0).unsqueeze(0)\n",
    "    \n",
    "    return A_logpred_probs.reshape(-1, *Alambda.size()).permute(1,2,0).exp()\n",
    "\n",
    "\n",
    "def getWeightedSquaredHistogramError(hist_counts, hist_pred_probs, dim=-1):\n",
    "    dim = dim if dim>0 else (hist_counts.ndimension()+dim)\n",
    "    N = hist_counts.sum(dim)\n",
    "    res_counts = hist_counts - hist_pred_probs*N.unsqueeze(dim)\n",
    "    expected_variance = N.unsqueeze(dim)*hist_pred_probs*(1.-hist_pred_probs)\n",
    "    \n",
    "    return (res_counts.pow(2)/expected_variance).sum(dim)\n",
    "\n",
    "\n",
    "import scipy.stats\n",
    "from preprocUtils import nansum\n",
    "def getChiSquaredHistogramError(hist_counts, hist_pred_probs, dim=-1, n_params = 1):\n",
    "    \"\"\"http://maxwell.ucsc.edu/~drip/133/ch4.pdf , eq.(3)\n",
    "    \n",
    "    Degrees of freedom (num_hist_bins - num_constraints of pred_probs)\n",
    "    \n",
    "    \"\"\"\n",
    "    dim = dim if dim>0 else (hist_counts.ndimension()+dim)\n",
    "    N = nansum(hist_counts, dim)\n",
    "    Nbins = (torch.isnan(hist_counts)==False).sum(dim)\n",
    "    res_counts = hist_counts - hist_pred_probs*N.unsqueeze(dim)\n",
    "    expected_value = N.unsqueeze(dim)*hist_pred_probs\n",
    "    \n",
    "    chi_squared_test_statistic = res_counts.pow(2)/expected_value\n",
    "    \n",
    "    chi_square_at_point05 = torch.tensor(scipy.stats.chi2.isf(0.05, Nbins-1- n_params))\n",
    "    \n",
    "    return nansum(chi_squared_test_statistic, dim), chi_square_at_point05\n",
    "\n",
    "\n",
    "def getLikelihoodRatioHistogramError(hist_counts, hist_pred_probs, dim=-1, n_params = 1):\n",
    "    \"\"\"So-called multinomial test\n",
    "    \n",
    "    Degrees of freedom (num_hist_bins - num_constraints of pred_probs)\n",
    "    \n",
    "    \"\"\"\n",
    "    dim = dim if dim>0 else (hist_counts.ndimension()+dim)\n",
    "    N = nansum(hist_counts, dim)\n",
    "    Nbins = (torch.isnan(hist_counts)==False).sum(dim)\n",
    "    hist_obs_probs = hist_counts/N.unsqueeze(dim)\n",
    "\n",
    "    lr_test_statistic = 2*hist_counts*(hist_obs_probs/hist_pred_probs).clamp(1e-10, 1e20).log()\n",
    "    \n",
    "    \n",
    "    df = int(hist_counts.size(dim))-1- n_params\n",
    "    \n",
    "    #correction_factor = 1. + hist_pred_probs.log().mul(-1.).logsumexp(dim) #Williams (1976)\n",
    "    \n",
    "    chi_square_at_point05 = torch.tensor(scipy.stats.chi2.isf(0.05, Nbins-1- n_params))\n",
    "    \n",
    "    return nansum(lr_test_statistic, dim), chi_square_at_point05\n",
    "    \n",
    "    \n",
    "def getKolmogorovSmirnovHistogramError(hist_counts, hist_pred_probs, dim=-1):\n",
    "    \"\"\"Kolmogorov-Smirnov test for goodness of fit\n",
    "    \n",
    "    two-sided test, implemented for histogram data, but test statistics and critical value computed as in scipy\n",
    "    https://github.com/scipy/scipy/blob/master/scipy/stats/stats.py#L4416\n",
    "    \"\"\"\n",
    "    dim = dim if dim>0 else (hist_counts.ndimension()+dim)\n",
    "    N = hist_counts.sum(dim)\n",
    "    hist_obs_cdf = hist_counts.cumsum(dim)/N.unsqueeze(dim)\n",
    "    hist_pred_cdf = hist_pred_probs.cumsum(dim)\n",
    "\n",
    "    ks_test_statistic = (hist_obs_cdf-hist_pred_cdf).abs().max(dim)[0]\n",
    "    \n",
    "    # Use approx mode for small sample\n",
    "  \n",
    "    return ks_test_statistic, (scipy.stats.distributions.kstwobign.sf(ks_test_statistic * N.sqrt())) #(2 * scipy.stats.distributions.ksone.sf(ks_test_statistic, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bins(hist_counts, hist_pred_probs, \n",
    "               target_expected_count = 5, \n",
    "               remove_first_bin = True,\n",
    "               adaptive_cats_based_on_lambda = 1,\n",
    "               hist_lambdas = torch.empty(0)\n",
    "              ):\n",
    "    \"\"\"\n",
    "    Ensure that on average the histogram bins have enough observations in them in expectation\n",
    "    Various rules of thumb:\n",
    "     - https://www.statsdirect.com/help/nonparametric_methods/chisq_goodness_fit.htm\n",
    "     \n",
    "     - D. S. Moore, G. P. McCabe, Introduction to the Practice of Statistics, W. H. Freeman Publishing Company, New York, 2007.\n",
    "    \n",
    "    (Expected frequency > 1 everywhere and expected frequency > 5 in 80% of the bins)\n",
    "    \n",
    "    Compute the corresponding probabilities as well in the merged bins\n",
    "    \"\"\"\n",
    "    \n",
    "    if adaptive_cats_based_on_lambda > 1:\n",
    "        lambda_cat_inds = torch.tensor(np.digitize(\n",
    "            hist_lambdas, \n",
    "            np.quantile(hist_lambdas.view(-1), np.linspace(0.0, 0.95, adaptive_cats_based_on_lambda)))-1, \n",
    "            #np.linspace(hist_lambdas.min(), hist_lambdas.max()+1, adaptive_cats_based_on_lambda+1))-1,\n",
    "                                      dtype=torch.uint8)\n",
    "    else:\n",
    "        lambda_cat_inds=torch.zeros_like(hist_counts[:,:,0])\n",
    "        \n",
    "    if remove_first_bin:\n",
    "        hist_counts = hist_counts[:,:,1:]\n",
    "        hist_pred_probs = hist_pred_probs[:,:,1:]\n",
    "        # Renormalise predictive probabilities\n",
    "        hist_pred_probs.div_(preprocUtils.nansum(hist_pred_probs, dim=2).unsqueeze(-1)) \n",
    "    \n",
    "    n = hist_counts.sum(2).view(-1)\n",
    "    hist_expected_freqs = n.unsqueeze(-1) * hist_pred_probs.view(-1, hist_pred_probs.size(-1))\n",
    "    \n",
    "    out_counts = torch.zeros_like(hist_counts)\n",
    "    out_pred_probs = torch.zeros_like(hist_pred_probs)\n",
    "\n",
    "    for cur_cat in range(0, adaptive_cats_based_on_lambda):\n",
    "        #print(\"Current category: {} \\n --------------\".format(cur_cat))\n",
    "        cur_pixels_mask = (lambda_cat_inds == cur_cat)\n",
    "\n",
    "        low_freqs = torch.tensor(\n",
    "            np.quantile(\n",
    "                hist_expected_freqs[cur_pixels_mask.view(-1)],\n",
    "                    q = 0.2, axis = 0)\n",
    "        )\n",
    "\n",
    "        # Can't use A[mask1][:,mask2] = ... as assignment (but also doesnt give error.)\n",
    "        tmp_counts = torch.zeros_like(out_counts[cur_pixels_mask])\n",
    "        tmp_pred_probs = torch.zeros_like(out_pred_probs[cur_pixels_mask])\n",
    "\n",
    "        cur_bin = 0\n",
    "        cur_merged_bin = 0\n",
    "        cur_expected_count = 0\n",
    "        while cur_bin < low_freqs.size(0):\n",
    "            #print(cur_bin)\n",
    "            cur_bin_low = cur_bin\n",
    "\n",
    "            cur_expected_count = low_freqs[cur_bin]\n",
    "            while (cur_expected_count < target_expected_count) and (cur_bin < (low_freqs.size(0)-1)):\n",
    "                cur_bin += 1\n",
    "                cur_expected_count += low_freqs[cur_bin]\n",
    "\n",
    "            tmp_counts[:,cur_merged_bin] += hist_counts[cur_pixels_mask][:, cur_bin_low:(cur_bin+1)].sum(1)\n",
    "            tmp_pred_probs[:,cur_merged_bin] += hist_pred_probs[cur_pixels_mask][:, cur_bin_low:(cur_bin+1)].sum(1)\n",
    "\n",
    "            cur_bin += 1\n",
    "            cur_merged_bin += 1\n",
    "\n",
    "\n",
    "        # Merge the last bin with the previous one if needed\n",
    "        if cur_expected_count < target_expected_count:\n",
    "            cur_merged_bin -= 1\n",
    "            tmp_counts[:,cur_merged_bin-1] += tmp_counts[:,cur_merged_bin]\n",
    "            tmp_pred_probs[:,cur_merged_bin-1] += tmp_pred_probs[:,cur_merged_bin]\n",
    "\n",
    "\n",
    "        tmp_counts[:,cur_merged_bin:] = float('nan')\n",
    "        tmp_pred_probs[:,cur_merged_bin:] = float('nan')\n",
    "\n",
    "        out_counts[cur_pixels_mask] = tmp_counts\n",
    "        out_pred_probs[cur_pixels_mask] = tmp_pred_probs\n",
    "        \n",
    "#     out_counts = out_counts[:,:,:cur_merged_bin]\n",
    "#     out_pred_probs = out_pred_probs[:,:, :cur_merged_bin]\n",
    "    \n",
    "    # Renormalise predictive probabilities\n",
    "    out_pred_probs.div_(preprocUtils.nansum(out_pred_probs, dim=2).unsqueeze(-1)) \n",
    "    \n",
    "    return out_counts, out_pred_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_counts = Ahists\n",
    "# hist_pred_probs = Apreds\n",
    "# hist_lambdas = imgsImputedLambda\n",
    "\n",
    "# adaptive_cats_based_on_lambda = 5\n",
    "# target_expected_count = 5.\n",
    "\n",
    "# if adaptive_cats_based_on_lambda > 1:\n",
    "#     lambda_cat_inds = torch.tensor(np.digitize(\n",
    "#         hist_lambdas, np.linspace(hist_lambdas.min(), hist_lambdas.max()+1, adaptive_cats_based_on_lambda+1))-1,\n",
    "#                                   dtype=torch.uint8)\n",
    "# else:\n",
    "#     lambda_cat_inds=torch.zeros_like(hist_counts[:,:,0])\n",
    "\n",
    "# if remove_first_bin:\n",
    "#     hist_counts = hist_counts[:,:,1:]\n",
    "#     hist_pred_probs = hist_pred_probs[:,:,1:]\n",
    "#     # Renormalise predictive probabilities\n",
    "#     hist_pred_probs.div_(preprocUtils.nansum(hist_pred_probs, dim=2).unsqueeze(-1)) \n",
    "\n",
    "# n = hist_counts.sum(2).view(-1)\n",
    "# hist_expected_freqs = n.unsqueeze(-1) * hist_pred_probs.view(-1, hist_pred_probs.size(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_counts = torch.zeros_like(hist_counts)\n",
    "# out_pred_probs = torch.zeros_like(hist_pred_probs)\n",
    "\n",
    "# for cur_cat in range(1, adaptive_cats_based_on_lambda):\n",
    "#     print(\"Current category: {} \\n --------------\".format(cur_cat))\n",
    "#     cur_pixels_mask = (lambda_cat_inds == cur_cat)\n",
    "\n",
    "#     low_freqs = torch.tensor(\n",
    "#         np.quantile(\n",
    "#             hist_expected_freqs[cur_pixels_mask.view(-1)],\n",
    "#                 q = 0.2, axis = 0)\n",
    "#     )\n",
    "\n",
    "#     # Can't use A[mask1][:,mask2] = ... as assignment (but also doesnt give error.)\n",
    "#     tmp_counts = torch.zeros_like(out_counts[cur_pixels_mask])\n",
    "#     tmp_pred_probs = torch.zeros_like(out_pred_probs[cur_pixels_mask])\n",
    "    \n",
    "#     cur_bin = 0\n",
    "#     cur_merged_bin = 0\n",
    "#     cur_expected_count = 0\n",
    "#     while cur_bin < low_freqs.size(0):\n",
    "#         print(cur_bin)\n",
    "#         cur_bin_low = cur_bin\n",
    "\n",
    "#         cur_expected_count = low_freqs[cur_bin]\n",
    "#         while (cur_expected_count < target_expected_count) and (cur_bin < (low_freqs.size(0)-1)):\n",
    "#             cur_bin += 1\n",
    "#             cur_expected_count += low_freqs[cur_bin]\n",
    "            \n",
    "#         tmp_counts[:,cur_merged_bin] += hist_counts[cur_pixels_mask][:, cur_bin_low:(cur_bin+1)].sum(1)\n",
    "#         tmp_pred_probs[:,cur_merged_bin] += hist_pred_probs[cur_pixels_mask][:, cur_bin_low:(cur_bin+1)].sum(1)\n",
    "\n",
    "#         cur_bin += 1\n",
    "#         cur_merged_bin += 1\n",
    "\n",
    "\n",
    "#     # Merge the last bin with the previous one if needed\n",
    "#     if cur_expected_count < target_expected_count:\n",
    "#         cur_merged_bin -= 1\n",
    "#         tmp_counts[:,cur_merged_bin-1] += tmp_counts[:,cur_merged_bin]\n",
    "#         tmp_pred_probs[:,cur_merged_bin-1] += tmp_pred_probs[:,cur_merged_bin]\n",
    "        \n",
    "\n",
    "#     tmp_counts[:,cur_merged_bin:] = float('nan')\n",
    "#     tmp_pred_probs[:,cur_merged_bin:] = float('nan')\n",
    "    \n",
    "#     out_counts[cur_pixels_mask] = tmp_counts\n",
    "#     out_pred_probs[cur_pixels_mask] = tmp_pred_probs\n",
    "    \n",
    "# # out_counts = out_counts[:,:,:cur_merged_bin]\n",
    "# # out_pred_probs = out_pred_probs[:,:, :cur_merged_bin]\n",
    "\n",
    "# # # Renormalise predictive probabilities\n",
    "# # out_pred_probs.div_(preprocUtils.nansum(out_pred_probs, dim=2).unsqueeze(-1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indeces = torch.zeros_like(imgsImputedLambda.cpu(), dtype=torch.uint8)\n",
    "train_indeces[train_x.long().unbind(1)] = 1\n",
    "test_indeces = torch.ones_like(imgsImputedLambda.cpu(), dtype=torch.uint8)\n",
    "test_indeces[train_x.long().unbind(1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = 0.\n",
    "#r1 = np.floor(float(imgsImputed.max()/2))\n",
    "r1 = float(imgsImputed.max())\n",
    "nbins = 1000\n",
    "\n",
    "Ahists, bin_centers = fast_histograms(imgsImputed.cuda(), r0, r1, nbins, output_device='cpu')\n",
    "#Ahists_orig, bin_centers = fast_histograms(imgs.cuda(), r0, r1, nbins, output_device='cpu')\n",
    "#Ahists_dense, bin_centers_dense = fast_histograms(imgsImputed.cuda(), r0, r1, nbins*5, output_device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apreds = fast_predictive_probs(imgsImputedLambda.cuda(), bin_centers.cuda(),\n",
    "                               photon_log_probs.cuda(), gray_levels.cuda(),\n",
    "                               batch_size = 50,\n",
    "                              output_device = 'cpu')\n",
    "\n",
    "# Apreds_dense = fast_predictive_probs(imgsImputedLambda.cuda(), bin_centers_dense.cuda(),\n",
    "#                                photon_log_probs.cuda(), gray_levels.cuda(),\n",
    "#                                batch_size = 50,\n",
    "#                               output_device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_chiErrors, chi_thres = getChiSquaredHistogramError(Ahists, Apreds, n_params = n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ksErrors, ks_thres = getKolmogorovSmirnovHistogramError(Ahists, Apreds, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(A_ksErrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(torch.tensor(ks_thres).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = 0 if use_validation_data_only else sum([p.numel() for p in likelihood.parameters()])\n",
    "\n",
    "A_chiErrors, chi_thres = getChiSquaredHistogramError(Ahists, Apreds, n_params = n_params)\n",
    "                                                     #n_params=sum([p.numel() for p in likelihood.parameters()]))\n",
    "\n",
    "# A_chiErrors_dense, chi_thres_dense = getChiSquaredHistogramError(Ahists_dense, Apreds_dense, \n",
    "#                                                      n_params=sum([p.numel() for p in likelihood.parameters()]))\n",
    "\n",
    "A_lrErrors, chi_thres = getLikelihoodRatioHistogramError(Ahists, Apreds, n_params = n_params)\n",
    "                                                     #n_params=sum([p.numel() for p in likelihood.parameters()]))\n",
    "\n",
    "# A_lrErrors_dense, chi_thres_dense = getLikelihoodRatioHistogramError(Ahists_dense, Apreds_dense, \n",
    "#                                                      n_params=sum([p.numel() for p in likelihood.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(A_chiErrors.clamp(min=chi_thres, max=chi_thres*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(A_lrErrors.clamp(min=chi_thres, max=chi_thres*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"{:.2f}% of pixels are well explained by a \n",
    "single underlying poisson process with a fixed rate over time, \n",
    "meaning there is no discernible time variation\"\"\".format(float((A_chiErrors<chi_thres).sum())/A_chiErrors.numel()*100)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind = np.ravel_multi_index((382, 103), imgsImputedLambda.size()) # order is y-x (as displayed in image)\n",
    "\n",
    "#ind = Apreds[:,:,9].argmax()\n",
    "#ind = Ahists[:,:,0].argmax()\n",
    "\n",
    "#ind = A_lrErrors.argmin()\n",
    "ind = A_chiErrors.argmin()\n",
    "#ind = A_chiErrors.argmax()\n",
    "\n",
    "#ind = imgsImputedLambda.view(-1).argmax()\n",
    "\n",
    "# tmp = fast_predictive_probs(imgsImputedLambda.view(-1)[ind].view(-1,1).cpu(), \n",
    "#                             bin_centers, photon_log_probs, gray_levels, dim = -1, batch_size=5, \n",
    "#                           output_device = None)\n",
    "\n",
    "# tmp1 = fast_predictive_probs(imgsImputedLambda.view(-1)[ind].view(-1,1).cpu(), \n",
    "#                             bin_centers, log_photon_prob_marginals, gray_levels, dim = -1, batch_size=5, \n",
    "#                           output_device = None)\n",
    "\n",
    "# tmp2 = likelihood.single_log_prob(torch.tensor([-18.]).expand(bin_centers.size()).view(1,-1), bin_centers,batchsize=200)\n",
    "# tmp2 -= tmp2.logsumexp(1)\n",
    "\n",
    "# tmp3 = likelihood.single_log_prob(imgsImputedLambda.view(-1)[ind].log().view(-1).cpu().expand(bin_centers.size()).view(1,-1), bin_centers,batchsize=200)\n",
    "# tmp3 -= tmp3.logsumexp(1)\n",
    "\n",
    "print(np.unravel_index(ind, imgsImputedLambda.size()), \n",
    "      imgsImputedLambda.view(-1)[ind],\n",
    "      \"\\n Chi test metric: {} < {} Threshold \".format(A_chiErrors.view(-1)[ind], chi_thres))\n",
    "plt(plot(Ahists.view(-1,Ahists.size(2))[ind,:].view(-1,1)/imgsImputed.size(2), bin_centers, now=False)\n",
    "    +plot(Apreds.view(-1,Apreds.size(2))[ind,:].view(-1,1), bin_centers, now=False)\n",
    "#     +plot(tmp.view(-1,1), bin_centers, now=False)\n",
    "#     +plot(tmp1.view(-1,1), bin_centers, now=False)\n",
    "#     +plot(tmp2.view(-1,1).exp(), bin_centers, now=False)\n",
    "#     +plot(tmp3.view(-1,1).exp(), bin_centers, now=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_counts, out_pred_probs = merge_bins(Ahists, Apreds, \n",
    "                                        target_expected_count=5., \n",
    "                                        remove_first_bin = True,\n",
    "                                       adaptive_cats_based_on_lambda = 200,\n",
    "                                       hist_lambdas = imgsImputedLambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = 0 if use_validation_data_only else sum([p.numel() for p in likelihood.parameters()])\n",
    "\n",
    "A_chiErrors_new, chi_thres_new = getChiSquaredHistogramError(out_counts, out_pred_probs, n_params = n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_thres_new.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(A_chiErrors_new.clamp(max=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(A_chiErrors_new>(chi_thres_new.float()*1.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = 0 if use_validation_data_only else sum([p.numel() for p in likelihood.parameters()])\n",
    "\n",
    "A_chiErrors_new, chi_thres_new = getChiSquaredHistogramError(out_counts, out_pred_probs, n_params = n_params)\n",
    "                                                     #n_params=sum([p.numel() for p in likelihood.parameters()]))\n",
    "\n",
    "# A_chiErrors_dense, chi_thres_dense = getChiSquaredHistogramError(Ahists_dense, Apreds_dense, \n",
    "#                                                      n_params=sum([p.numel() for p in likelihood.parameters()]))\n",
    "\n",
    "A_lrErrors_new, chi_thres_new = getLikelihoodRatioHistogramError(out_counts, out_pred_probs, n_params = n_params)\n",
    "                                                     #n_params=sum([p.numel() for p in likelihood.parameters()]))\n",
    "\n",
    "# A_lrErrors_dense, chi_thres_dense = getLikelihoodRatioHistogramError(Ahists_dense, Apreds_dense, \n",
    "#                                                      n_params=sum([p.numel() for p in likelihood.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_thres_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_chiErrors_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(A_chiErrors_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(A_chiErrors_new.clamp(min=30, max=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(A_chiErrors_new.clamp(min=chi_thres_new, max=chi_thres_new*3))\n",
    "imagesc(A_lrErrors_new.clamp(min=chi_thres_new, max=chi_thres_new*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"{:.2f}% of pixels are well explained by a \n",
    "single underlying poisson process with a fixed rate over time, \n",
    "meaning there is no discernible time variation\"\"\".format(float((A_chiErrors_new<chi_thres_new).sum())/A_chiErrors_new.numel()*100)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind = np.ravel_multi_index((382, 103), imgsImputedLambda.size()) # order is y-x (as displayed in image)\n",
    "\n",
    "#ind = Apreds[:,:,9].argmax()\n",
    "ind = Ahists[:,:,9].argmax()\n",
    "\n",
    "#ind = A_lrErrors.argmin()\n",
    "#ind = A_chiErrors.argmin()\n",
    "#ind = A_lrErrors_new.argmin()\n",
    "ind = A_chiErrors_new.argmax()\n",
    "\n",
    "#ind = imgsImputedLambda.view(-1).argmax()\n",
    "\n",
    "# tmp = fast_predictive_probs(imgsImputedLambda.view(-1)[ind].view(-1,1).cpu(), \n",
    "#                             bin_centers, photon_log_probs, gray_levels, dim = -1, batch_size=5, \n",
    "#                           output_device = None)\n",
    "\n",
    "# tmp1 = fast_predictive_probs(imgsImputedLambda.view(-1)[ind].view(-1,1).cpu(), \n",
    "#                             bin_centers, log_photon_prob_marginals, gray_levels, dim = -1, batch_size=5, \n",
    "#                           output_device = None)\n",
    "\n",
    "# tmp2 = likelihood.single_log_prob(torch.tensor([-18.]).expand(bin_centers.size()).view(1,-1), bin_centers,batchsize=200)\n",
    "# tmp2 -= tmp2.logsumexp(1)\n",
    "\n",
    "# tmp3 = likelihood.single_log_prob(imgsImputedLambda.view(-1)[ind].log().view(-1).cpu().expand(bin_centers.size()).view(1,-1), bin_centers,batchsize=200)\n",
    "# tmp3 -= tmp3.logsumexp(1)\n",
    "\n",
    "# print(np.unravel_index(ind, imgsImputedLambda.size()), \n",
    "#       imgsImputedLambda.view(-1)[ind],\n",
    "#       \"\\n Chi test metric: {} \\n  LR test metric: {} \\n       Threshold: {}\".format(\n",
    "#           A_chiErrors_new.view(-1)[ind], A_lrErrors_new.view(-1)[ind], chi_thres_new))\n",
    "plt(plot(out_counts.view(-1,out_counts.size(2))[ind,:].view(-1,1)/imgsImputed.size(2)*500, now=False)\n",
    "    +plot(out_pred_probs.view(-1,out_pred_probs.size(2))[ind,:].view(-1,1)*500, now=False)\n",
    "#     +plot(tmp.view(-1,1), bin_centers, now=False)\n",
    "#     +plot(tmp1.view(-1,1), bin_centers, now=False)\n",
    "#     +plot(tmp2.view(-1,1).exp(), bin_centers, now=False)\n",
    "#     +plot(tmp3.view(-1,1).exp(), bin_centers, now=False)\n",
    ")\n",
    "\n",
    "print(np.unravel_index(ind, imgsImputedLambda.size()), \n",
    "      imgsImputedLambda.view(-1)[ind],\n",
    "      \"\\n Chi test metric: {} \\n  LR test metric: {} \\n    Threshold: {}\".format(\n",
    "          A_chiErrors.view(-1)[ind], A_lrErrors.view(-1)[ind], chi_thres))\n",
    "plt(plot(Ahists.view(-1,Ahists.size(2))[ind,:].view(-1,1)/imgsImputed.size(2), bin_centers, now=False)\n",
    "    +plot(Apreds.view(-1,Apreds.size(2))[ind,:].view(-1,1), bin_centers, now=False)\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ind = Apreds[:,:,9].argmax()\n",
    "# ind = Ahists[:,:,1].argmax()\n",
    "\n",
    "# #ind = A_lrErrors.argmax()\n",
    "# #ind = A_chiErrors.argmin()\n",
    "\n",
    "# #ind = imgsImputedLambda.view(-1).argmin()\n",
    "\n",
    "# tmp2 = likelihood.single_log_prob(torch.tensor([-18.]).expand(bin_centers.size()).view(1,-1), bin_centers,batchsize=200)\n",
    "# tmp2 -= tmp2.logsumexp(1)\n",
    "\n",
    "# tmp3 = likelihood.single_log_prob(imgsImputedLambda.view(-1)[ind].log().view(-1).cpu().expand(bin_centers.size()).view(1,-1), bin_centers,batchsize=200)\n",
    "# tmp3 -= tmp3.logsumexp(1)\n",
    "\n",
    "# print(imgsImputedLambda.view(-1)[ind])\n",
    "# plt(plot(Ahists.view(-1,Ahists.size(2))[ind,:].view(-1,1)/imgsImputed.size(2), bin_centers, now=False)\n",
    "#     +plot(Apreds.view(-1,Apreds.size(2))[ind,:].view(-1,1), bin_centers, now=False)\n",
    "#     +plot(tmp2.view(-1,1).exp(), bin_centers, now=False)\n",
    "#     +plot(tmp3.view(-1,1).exp(), bin_centers, now=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = Apreds_dense[:,:,1].argmax()\n",
    "# #ind = A_lrErrors.argmax()\n",
    "# #ind = A_chiErrors.argmin()\n",
    "\n",
    "# #ind = imgsImputedLambda.view(-1).argmin()\n",
    "\n",
    "# tmp = fast_predictive_probs(imgsImputedLambda.view(-1)[ind].view(-1,1).cpu(), bin_centers_dense, log_photon_prob_marginals, gray_levels, dim = -1, batch_size=5, \n",
    "#                           output_device = None)\n",
    "# #tmp -= tmp.logsumexp(1)\n",
    "\n",
    "# tmp2 = likelihood.single_log_prob(torch.tensor([-18.]).expand(bin_centers_dense.size()).view(1,-1), bin_centers_dense,batchsize=200)\n",
    "# tmp2 -= tmp2.logsumexp(1)\n",
    "\n",
    "# tmp3 = likelihood.single_log_prob(imgsImputedLambda.view(-1)[ind].log().view(-1).cpu().expand(bin_centers_dense.size()).view(1,-1), bin_centers_dense,batchsize=200)\n",
    "# tmp3 -= tmp3.logsumexp(1)\n",
    "\n",
    "# print(imgsImputedLambda.view(-1)[ind])\n",
    "# plt(plot(Ahists_dense.view(-1,Ahists_dense.size(2))[ind,:].view(-1,1)/imgsImputed.size(2), bin_centers_dense, now=False)\n",
    "#     +plot(tmp.view(-1,1), bin_centers_dense, now=False)\n",
    "#     +plot(Apreds_dense.view(-1,Apreds_dense.size(2))[ind,:].view(-1,1), bin_centers_dense, now=False)\n",
    "#     +plot(tmp2.view(-1,1).exp(), bin_centers_dense, now=False)\n",
    "#     +plot(tmp3.view(-1,1).exp(), bin_centers_dense, now=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_edges_dense = torch.cat([bin_centers_dense[0].view(-1)-1e10,\n",
    "#                        bin_centers_dense[1:-1] - (bin_centers_dense[2]-bin_centers_dense[1]).div(2)+1e-7,\n",
    "#                        bin_centers_dense[-1].view(-1),\n",
    "#                        bin_centers_dense[-1].view(-1)+1e10\n",
    "#                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r0=0.\n",
    "# r1=400.\n",
    "# bin_centers_dense = torch.arange(r0,r1, 1.)\n",
    "# bin_edges_dense = torch.cat([torch.tensor([r0])-1e10,\n",
    "#                        bin_centers_dense[1:-1],\n",
    "#                        torch.tensor([r1])+1e10\n",
    "#                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#float((imgs!=imgsImputed).sum())/imgs.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist, bins = np.histogram(imgsImputed.detach().cpu(), bin_edges_dense)\n",
    "# hist_orig, bins = np.histogram(imgs.detach().cpu(), bin_edges_dense)\n",
    "\n",
    "# plt(([plt_type.Scatter(y=hist/sum(hist), x=bin_centers_dense)]\n",
    "#      +[plt_type.Scatter(y=hist_orig/sum(hist_orig), x=bin_centers_dense)]\n",
    "#      +plot(Ahists_dense.sum(1).sum(0).view(-1,1).div(Ahists_dense.sum()), bin_centers_dense, now=False))\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt(plot(Ahists.sum(1).sum(0).view(-1,1).div(Ahists.sum()), bin_centers, now=False)\n",
    "#          +plot(tmp3.view(-1,1).exp(), bin_centers, now=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt(plot(Ahists_dense.sum(1).sum(0).view(-1,1).div(Ahists_dense.sum()), bin_centers_dense, now=False)\n",
    "#          +plot(tmp3.view(-1,1).exp(), bin_centers_dense, now=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ahists.view(-1,Ahists.size(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(A_chiErrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get histrograms of lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def createLocalHists(imgs, n_hist_grid = 3, nbins = None):\n",
    "    \"\"\"\n",
    "    Creates n_hist_grid**2 histograms tiling the spacial axis of the image\n",
    "    \n",
    "    ..note:\n",
    "        Do the whole thing on cpu, as histc is not supported on GPU and numpy hist is slow\n",
    "    \"\"\"\n",
    "    input_device = imgs.device\n",
    "    ranges = list()\n",
    "    for d in range(2):\n",
    "        ranges.append(list())\n",
    "        tmp = torch.linspace(0, imgs.shape[d],n_hist_grid+1).round().cpu()\n",
    "        for n in range(n_hist_grid):\n",
    "            ranges[d].append(slice(int(tmp[n]), int(tmp[n+1])))\n",
    "        \n",
    "    imgs_max = float(imgs.max())\n",
    "    imgs_min = float(imgs.min())\n",
    "    n_bins = int(nbins) if nbins is not None else int(imgs_max-imgs_min+1.)\n",
    "    \n",
    "    # Modify min and max such that the bin centers actually start on imgs_max and imgs_min \n",
    "    bin_edge_correction = (imgs_max-imgs_min)/(2.*float(n_bins-1.))\n",
    "    \n",
    "    out = torch.stack([\n",
    "            torch.histc(imgs[ind0, ind1, :].cpu(), # histc does not support GPU\n",
    "                bins = n_bins,\n",
    "                min = imgs_min-bin_edge_correction,\n",
    "                max = imgs_max+bin_edge_correction)\n",
    "                     for ind0, ind1 in itertools.product(*ranges)\n",
    "        ],\n",
    "        dim = 1)\n",
    "    \n",
    "    hist_bin_centers = torch.linspace(imgs_min, imgs_max, n_bins)\n",
    "    \n",
    "    return out.to(input_device), hist_bin_centers.to(input_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def summariseHists(histOut, numQuantXs = 101, retCdf = False, invNormalise=True):\n",
    "    \"\"\"\n",
    "    Summarises the historgrams by their quantile function\n",
    "    and (normalised) inverse quantile functions with \n",
    "        numQuantXs unique inverse quantile X points linspaced between 0 and 1 (inclusive)\n",
    "        We later assume a piecewise-constant approximation of the true quantile function with these points being centers\n",
    "        Set numQuantXs = None to avoid the approximation (the full cdf resolution is used then)\n",
    "    \"\"\"\n",
    "    input_device = histOut[0].device\n",
    "    \n",
    "    # Get quantile function as OrderedDict ('y' - matrix, 'x' - vector)\n",
    "    cdfs = OrderedDict( [ # init input is list of tuples\n",
    "        ('y', torch.cumsum(histOut[0], dim=0)/histOut[0].sum(0)),\n",
    "        ('x', histOut[1])\n",
    "        ])\n",
    "    \n",
    "    if retCdf:\n",
    "        return cdfs\n",
    "    \n",
    "    # Get quantile function (inverse cdf) as OrderedDict ('y' - matrix, 'x' - vector)\n",
    "    \n",
    "    numQuantXs = numQuantXs if numQuantXs is not None else len(histOut[1])\n",
    "    \n",
    "    inv_bins = torch.linspace(0., 1., numQuantXs).to(input_device)\n",
    "    div_by = len(histOut[1]) if invNormalise else 1.\n",
    "    quantFuncs = OrderedDict( [\n",
    "            ('y', (cdfs['y'].unsqueeze(-1) - inv_bins.view(1,1,-1)).abs().argmin(0).float().permute(1,0).div(float(div_by))),\n",
    "            ('x', inv_bins)\n",
    "            ])\n",
    "    \n",
    "    return quantFuncs\n",
    "    \n",
    "    # Get non-normalised inverse quantile function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distWasserstein(quantFuncs, p=2., retPwDistsWeighted = False):\n",
    "    # Get pairwise distances at every point\n",
    "    pwDists = (quantFuncs['y'].unsqueeze(2) - quantFuncs['y'].unsqueeze(1)).abs()\n",
    "    \n",
    "    xbinSize = quantFuncs['x'][1] - quantFuncs['x'][0]\n",
    "    pwDistsWeighted = xbinSize * (pwDists.pow(p))\n",
    "    \n",
    "    if retPwDistsWeighted:\n",
    "        return pwDistsWeighted\n",
    "    \n",
    "    return pwDistsWeighted.sum(0).pow(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of chi_squared test statistics\n",
    "histOut = createLocalHists(A_chiErrors.cpu()[train_indeces].view(-1,1,1).clamp(max=chi_thres*4),1, nbins=1000)\n",
    "histOut2 = createLocalHists(A_chiErrors.cpu()[test_indeces].view(-1,1,1).clamp(max=chi_thres*4),1, nbins=1000)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of likelihood ratio test statistics\n",
    "histOut = createLocalHists(A_lrErrors.cpu()[train_indeces].view(-1,1,1),1, nbins=1000)\n",
    "histOut2 = createLocalHists(A_lrErrors.cpu()[test_indeces].view(-1,1,1),1, nbins=1000)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of corrected vs uncorrected lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsImputedLambdaCorrected = imgsImputedLambda/(pred_gain_func.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train vs test uncorrected lambda\n",
    "histOut = createLocalHists(imgsImputedLambda[train_indeces].view(-1,1,1),1, nbins=100)\n",
    "histOut2 = createLocalHists(imgsImputedLambda[test_indeces].view(-1,1,1),1, nbins=100)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrected vs corrected lambda\n",
    "histOut = createLocalHists(imgsImputedLambda.view(-1,1,1),1, nbins=100)\n",
    "histOut2 = createLocalHists(imgsImputedLambdaCorrected.view(-1,1,1),1, nbins=100)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrected vs corrected training lambda\n",
    "histOut = createLocalHists(imgsImputedLambda[train_indeces].view(-1,1,1),1, nbins=100)\n",
    "histOut2 = createLocalHists(imgsImputedLambdaCorrected[train_indeces].view(-1,1,1),1, nbins=100)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected train vs test lambda\n",
    "histOut = createLocalHists(imgsImputedLambdaCorrected[train_indeces].view(-1,1,1),1, nbins=100)\n",
    "histOut2 = createLocalHists(imgsImputedLambdaCorrected[test_indeces].view(-1,1,1),1, nbins=100)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrected vs corrected test lambda\n",
    "histOut = createLocalHists(imgsImputedLambda[test_indeces].view(-1,1,1),1, nbins=100)\n",
    "histOut2 = createLocalHists(imgsImputedLambdaCorrected[test_indeces].view(-1,1,1),1, nbins=100)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of corrected lambdas with low and high LR test statistics\n",
    "inds1 = A_lrErrors<chi_thres\n",
    "inds2 = A_lrErrors>=chi_thres\n",
    "\n",
    "# Uncorrected vs corrected test lambda\n",
    "histOut = createLocalHists(imgsImputedLambdaCorrected[inds1].view(-1,1,1),1, nbins=100)\n",
    "histOut2 = createLocalHists(imgsImputedLambdaCorrected[inds2].view(-1,1,1),1, nbins=100)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of corrected lambdas with low and high Chi test statistics\n",
    "inds1 = A_chiErrors<chi_thres\n",
    "inds2 = A_chiErrors>=chi_thres\n",
    "\n",
    "# Uncorrected vs corrected test lambda\n",
    "histOut = createLocalHists(imgsImputedLambdaCorrected[inds1].view(-1,1,1),1, nbins=100)\n",
    "histOut2 = createLocalHists(imgsImputedLambdaCorrected[inds2].view(-1,1,1),1, nbins=100)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of corrected lambdas with low and high Chi test statistics\n",
    "inds1 = A_chiErrors<chi_thres\n",
    "inds2 = A_chiErrors>=chi_thres\n",
    "\n",
    "# Uncorrected vs corrected test lambda\n",
    "histOut = createLocalHists(imgsImputedLambda[inds1].view(-1,1,1),1, nbins=100)\n",
    "histOut2 = createLocalHists(imgsImputedLambda[inds2].view(-1,1,1),1, nbins=100)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histOut = createLocalHists(imgsImputedLambda.view(-1,1,1),1, nbins=100)\n",
    "histOut2 = createLocalHists(tmp.view(-1,1,1),1, nbins=100)\n",
    "plt(plot(histOut[0], histOut[1], now=False)+plot(histOut2[0], histOut2[1], now=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sumHistOut['y'], sumHistOut['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histOut = createLocalHists(Aerrors[train_indeces].view(-1,1,1),1, nbins=100)\n",
    "histOut2 = createLocalHists(Aerrors[test_indeces].view(-1,1,1),1, nbins=100)\n",
    "plt((plot(histOut[0]/histOut[0].sum()/(histOut[1][1]-histOut[1][0]), histOut[1], now=False)\n",
    "    +plot(histOut2[0]/histOut2[0].sum()/(histOut2[1][1]-histOut2[1][0]), histOut2[1], now=False))\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getLambdaPoly(w):\n",
    "#     #x = torch.arange(w.size(dim)).float().view(*([1]*max(dim,0))+[-1]+[1]*max(w.ndimension()-dim-1,0))\n",
    "#     x = torch.arange(w.numel()).float()\n",
    "    \n",
    "#     # Get coeffs for coeff[0]*lambda^0, coeff[1]*lambda, ... coeff[N]*lambda^N\n",
    "#     coeffs = (w[:-1]-w[1:]).div((x[:-1]+1).lgamma().exp())\n",
    "#     coeffs = torch.cat([coeffs.view(-1), -w[-1].div((x[-1]+1).lgamma().exp()).view(1)])\n",
    "    \n",
    "#     return coeffs\n",
    "    \n",
    "#     # Get rid of unnecessary too small coeffs\n",
    "# #     max_coeff = coeffs.abs().max()\n",
    "# #     coeffs[coeffs.abs()<(1e-3*max_coeff)] = 0\n",
    "\n",
    "#     # Get rid of zero coeffs (numerical issues)\n",
    "#     coeffs = coeffs[:int((coeffs>0).max(0)[1]+1)]\n",
    "    \n",
    "#     res = np.roots(np.flip(coeffs.detach()))\n",
    "    \n",
    "#     return res\n",
    "    \n",
    "#     res = res[np.isreal(res)]\n",
    "#     res = np.real(res)\n",
    "#     res = res[res>1e-9]\n",
    "    \n",
    "#     res = torch.tensor(res)\n",
    "#     if res.numel()==0:\n",
    "#         res = torch.tensor([0.])\n",
    "    \n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "imagesc(mean_im, pixels_per_micron=1.15)\n",
    "imagesc(corr_mean_im, pixels_per_micron=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(corr_mean_im.clamp(0,5), pixels_per_micron=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(mll.pmGa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the whole imgsImputes\n",
    "# gainRange = [1e-20, float('inf')]\n",
    "# imgsImputedCorr = ((imgsImputed-likelihood.offset)\n",
    "#                    .div(torch.clamp(pred_gain_func, min=1./gainRange[1], max=1./gainRange[0]).unsqueeze(-1))\n",
    "#                    +likelihood.offset\n",
    "#                   ).clamp(min=0.)\n",
    "\n",
    "gainRange = [1e-20, float('inf')]\n",
    "imgsImputedCorr = ((imgsImputed)\n",
    "                   .div(torch.clamp(pred_gain_func, min=1./gainRange[1], max=1./gainRange[0]).unsqueeze(-1))\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(imgsImputedCorr.mean(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive kernel decomposition\n",
    "Vincent Adam's paper MLSP 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gain_func_orig = copy.deepcopy(pred_gain_func.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(model.covar_module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMarginals(model):\n",
    "    \"\"\"\n",
    "    Given a model with 0 mean a scaled additive kernel as its covar_module, \n",
    "    returns the marginal contributions of kernels.\n",
    "    \n",
    "    Also assumes kernels[0] is symmetrized and thus non-invertible. \n",
    "    \"\"\"\n",
    "    \n",
    "    inp = model.inducing_points\n",
    "    kernels = model.covar_module.base_kernel.kernels\n",
    "    \n",
    "    Ks = [k(inp).evaluate()*model.covar_module.log_outputscale.exp() \n",
    "         for k in kernels]\n",
    "    Ksum = sum(Ks)\n",
    "    \n",
    "    \n",
    "    muF = model.variational_output().mean()\n",
    "    if isinstance(model.mean_module, gpytorch.means.ConstantMean):\n",
    "        muF = muF-model.mean_module.constant.data.squeeze()\n",
    "    \n",
    "    \n",
    "    sigF = model.variational_output().covar().evaluate()\n",
    "    muF_Scaled = sigF.inverse().matmul(muF)\n",
    "    \n",
    "    Ktilde = (sigF.inverse() - Ksum.inverse()).inverse() + Ksum\n",
    "    Ktilde_inv = Ktilde.inverse()\n",
    "    \n",
    "    Vs = [[ Kd1 - Kd1.matmul(Ktilde_inv).matmul(Kd2)\n",
    "        \n",
    "    for Kd1 in Ks]\n",
    "    for Kd2 in Ks]\n",
    "    \n",
    "    Ksum_Scaled = Ktilde_inv.matmul(Ksum)\n",
    "    \n",
    "    nus = [(Kd - Kd.matmul(Ksum_Scaled)).matmul(muF_Scaled)\n",
    "           for Kd in Ks]\n",
    "    \n",
    "    # Add back the prior mean to component 1\n",
    "    nus[1] = nus[1]\n",
    "           \n",
    "    margRandVars = [\n",
    "        gpytorch.random_variables.GaussianRandomVariable(\n",
    "        nus[i].view(-1),\n",
    "        Vs[i][i])\n",
    "        #gpytorch.utils.pivoted_cholesky.pivoted_cholesky(Vs[i][i], 10))\n",
    "        for i in range(len(nus))]\n",
    "    \n",
    "    return nus, Vs, margRandVars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolateFromMarginal(model, randVar, inputs):\n",
    "    interp_indices, interp_values = model._compute_grid(inputs)\n",
    "    \n",
    "    # Compute test mean\n",
    "    # Left multiply samples by interpolation matrix\n",
    "    test_mean = gpytorch.utils.left_interp(interp_indices, interp_values, randVar.mean().unsqueeze(-1))\n",
    "    test_mean = test_mean.squeeze(-1)\n",
    "\n",
    "    # Compute test covar\n",
    "    test_covar = gpytorch.lazy.InterpolatedLazyVariable(\n",
    "        randVar.covar(), interp_indices, interp_values, interp_indices, interp_values\n",
    "    )\n",
    "        \n",
    "    return gpytorch.random_variables.GaussianRandomVariable(test_mean, test_covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nus, Vs, margRandVars = getMarginals(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "n_test_grid = torch.tensor(mean_im.shape)\n",
    "test_x = preprocUtils.create_test_grid(n_test_grid, ndims=2, device=device, a=dataStats['x_minmax'][0,:], b=dataStats['x_minmax'][1,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logGainFunc = interpolateFromMarginal(model, margRandVars[0], test_x)\n",
    "\n",
    "pred_gain_func = logGainFunc.mean().reshape(*n_test_grid).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logZFunc = interpolateFromMarginal(model, margRandVars[1], test_x)\n",
    "pred_Z_func = logZFunc.mean().reshape(*n_test_grid).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(pred_gain_func_orig)\n",
    "imagesc(pred_gain_func * pred_Z_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(pred_gain_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(pred_Z_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Showing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = '0', prior='noPrior', lik='linLik', stamp = '_00_firstRun_noPCremoved')\n",
    "\n",
    "imagesc(pred_gain_func, pixels_per_micron=1.15, heatmap=dict(colorscale='div'), image='svg', filename='fig1-res1-gain_noprior_linlik')\n",
    "imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_noprior_linlik')\n",
    "\n",
    "\n",
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = '0', prior='mexRadPrior', lik='linLik', stamp = '_00_firstRun_noPCremoved')\n",
    "\n",
    "imagesc(pred_gain_func, pixels_per_micron=1.15, heatmap=dict(colorscale='div'), image='svg', filename='fig1-res1-gain_mexRadprior_lik')\n",
    "imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_mexRadprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(corr_mean_im, pixels_per_micron=1.15, image='svg', filename='fig1-res1-corr_mexRadprior_lik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = '0', prior='noPrior', lik='unampLik', stamp = '_00_firstRun_noPCremoved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = '0', prior='mexRadPrior', lik='unampLik', stamp = '_00_firstRun_noPCremoved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = '0', prior='mexRadPrior', lik='linLik', stamp = '_00_firstRun_noPCremoved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = '0', prior='noPrior', lik='unampLik', stamp = '_00_firstRun_noPCremoved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = '0', prior='noPrior', lik='linLik', stamp = '_00_firstRun_noPCremoved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "display_results(retVars=True,data_id = '0', prior='noPrior', lik='linLik', stamp = '_00_firstRun_noPCremoved_smallLinNoiseInit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "imagesc(mean_im, pixels_per_micron=1.15)\n",
    "imagesc(corr_mean_im, pixels_per_micron=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagesc(pred_gain_func, colorscale='felfire')\n",
    "# imagesc(mean_im, image='svg', filename='fig1-mean_im', pixels_per_micron=1.15)\n",
    "# imagesc(corr_mean_im, image='svg', filename='fig1-mean_im_corr', pixels_per_micron=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(corr_mean_im, image='svg', filename='fig1-mean_im_corr', pixels_per_micron=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating photon probability from grey level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_levels = torch.arange(1e-4, 1.,1e-4) # In photon\n",
    "model_out = gpytorch.random_variables.GaussianRandomVariable(light_levels.log(), gpytorch.lazy.DiagLazyVariable(1e-7*torch.ones_like(light_levels)))\n",
    "lik_out = likelihood(model_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photon_counts, photon_log_probs = (\n",
    "    likelihood.getPhotonLogProbs(model_out.mean().view(-1,1).exp(), max_photon=float(20), reNormalise = False))\n",
    "p_PM = likelihood.createResponseDistributions(photon_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_levels = torch.arange(-50., 500.,20.)\n",
    "# tmp = likelihood.getLogProbSumOverTargetSamples(p_PM, gray_levels.view(-1))\n",
    "# plot(tmp.exp().div(tmp.exp().sum(1).view(-1,1)), gray_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = likelihood.single_log_prob(torch.arange(0.,20.).view(1,-1)*torch.ones_like(gray_levels).view(-1,1), gray_levels.view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tmp.exp().div(tmp.exp().sum(1).view(-1,1)).detach().data\n",
    "n_max = 5\n",
    "fig = plotStacked(torch.cat([a[:,:n_max], (1.-a[:,:n_max].sum(1)).view(-1,1)],dim=1), gray_levels, now=False)\n",
    "plt(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportFigure(fig, image='svg', filename='fig1-photon_cum_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt(plt_type.Figure(data=data, \n",
    "                layout=plt_type.Layout(\n",
    "                    xaxis=dict(\n",
    "                        title='Grey level in data'\n",
    "                    ),\n",
    "                    yaxis=dict(\n",
    "                        title='Cumulative probability of photon count'\n",
    "                    )\n",
    "                ))\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.exp().div(tmp.exp().sum(1).view(-1,1)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(lik_out.var().sqrt().view(-1,1), light_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(lik_out.mean().view(-1,1), light_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose non-approximate likelihood and posterior peakiness\n",
    "min_photon = 0\n",
    "max_photon = None\n",
    "import numpy as np\n",
    "for n_train_ind in [44323, 13450, 6792]:# 38150, 38155, 38159]:#44323, 17500, 13450, 6792]: # 6792 is a good training index, looks bumpy\n",
    "\n",
    "    hist, bins, bins_extended = croppedHist(train_y[n_train_ind,:], bins = 100)\n",
    "    model_out = model(train_x[n_train_ind,:].unsqueeze(0))\n",
    "    print(model_out.mean().exp())\n",
    "    print(likelihood.log_probability(model_out, train_y[n_train_ind, :]))\n",
    "    if likelihood.__class__==preprocLikelihoods.LinearGainLikelihood:\n",
    "        lik_out = likelihood(model_out)\n",
    "    else:\n",
    "        lik_out = likelihood(model_out, approx=False)\n",
    "    \n",
    "    # Sample from the output (we don't have access to log_prob for Mixture)\n",
    "    n_samples = 1e5*1.\n",
    "    if max_photon is None:\n",
    "        lik_sample = lik_out.sample(int(n_samples))\n",
    "    else:\n",
    "        curMixWeights = lik_out.rand_vars[0].weights[min_photon:max_photon+1] / lik_out.rand_vars[0].weights[min_photon:max_photon+1].sum()\n",
    "        lik_sample = preprocRandomVariables.MixtureRandomVariableWithSampler(\n",
    "            *lik_out.rand_vars[0].rand_vars[:max_photon+1], weights = curMixWeights.view(-1)).sample(int(n_samples))\n",
    "        \n",
    "    lik_hist = np.histogram(lik_sample.detach().cpu(), bins_extended)[0]/n_samples*train_y.size(1)\n",
    "    \n",
    "    #cur_out_moved = likelihood(model(train_x[n_train_ind,:].unsqueeze(0)+0.2)).cpu()\n",
    "    plt([plt_type.Bar(x=bins[:-1], y=hist),\n",
    "         plt_type.Scatter(x=bins[:-1], \n",
    "                          y=lik_hist, mode='lines+markers'),\n",
    "         plt_type.Scatter(x=torch.cat([model_out.mean().exp()*likelihood.log_gain.exp()]*2), \n",
    "                          y=np.array([0,30.]), mode='lines')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good data - model pairs\n",
    "\"\"\"\n",
    "dataset_name = 'neurofinder.04.00'\n",
    "tmp = scipy.io.loadmat(data_dir+dataset_name+'/imputed_first_700_frames.mat')\n",
    "imgsImputed = torch.tensor(tmp['imgsImputed'])\n",
    "mll = torch.load(\"savedModels/mll_20180814T170124\").cpu()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get numerical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = model(test_x)\n",
    "bg_mean = likelihood.forward(model_out).mean().reshape(*n_test_grid)\n",
    "\n",
    "imgsImputedMeanCorr = ((imgsImputed)\n",
    "                   .sub(bg_mean.unsqueeze(-1))\n",
    "                  )\n",
    "\n",
    "gainRange = [1e-20, float('inf')]\n",
    "imgsImputedCorr = ((imgsImputedMeanCorr)\n",
    "                   .div(torch.clamp(pred_gain_func, min=1./gainRange[1], max=1./gainRange[0]).unsqueeze(-1))\n",
    "                  )\n",
    "\n",
    "imgsImputedOrigCorr = ((imgsImputed)\n",
    "                   .div(torch.clamp(pred_gain_func_orig, min=1./gainRange[1], max=1./gainRange[0]).unsqueeze(-1))\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toMat(A):\n",
    "    A = A.sub(A.min())\n",
    "    A = A.div(A.max())\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbimporter import NotebookLoader\n",
    "scatterHex = NotebookLoader().load_module(\"preprocVisualisationTesting\").scatterHex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc((imgsImputed.std(2).pow(2)/imgsImputed.mean(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc((imgsImputed.std(2).pow(2)/imgsImputed.mean(2)).clamp(70,170))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(imgsImputedOrigCorr.std(2).pow(2)/imgsImputedOrigCorr.mean(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(imgsImputedCorr.std(2).pow(2)/imgsImputedCorr.mean(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(imgsImputedCorr.std(2).pow(2).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = model(test_x)\n",
    "bg1 = likelihood.forward(logZFunc)\n",
    "bg2 = likelihood.forward(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(bg2.mean().reshape(*n_test_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(bg1.mean().reshape(*n_test_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc((imgsImputed.mean(2)-bg2.mean().reshape(*n_test_grid))/pred_gain_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(imgsImputed.mean(2)-bg1.mean().reshape(*n_test_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want var / mean to be same everywhere (that would mean normalised gain?)\n",
    "# Compute Geometric mean of var/mean:\n",
    "# exp(1/n sum(log(var)-log(mean))) locally, show that corrected image is more stereotypical\n",
    "\n",
    "cur_filter = torch.ones(71,71)\n",
    "cur_filter = cur_filter.div(cur_filter.sum()).view(1,1,*cur_filter.size())\n",
    "\n",
    "xslice = slice(20,490)#slice(0,512)#slice(50,470)#\n",
    "yslice = slice(0,512)#slice(20,490)#slice(20,490)#\n",
    "for cur_imgs in [imgsImputed[yslice,xslice], imgsImputedOrigCorr[yslice,xslice]]:\n",
    "    logVars = torch.nn.functional.conv2d(\n",
    "        cur_imgs.std(2).pow(2).log().view(1,1,*cur_imgs.shape[:2]),\n",
    "        cur_filter,\n",
    "        padding=tuple((torch.tensor(cur_filter.shape)[2:]-1)/2)\n",
    "    )\n",
    "\n",
    "    logMeans = torch.nn.functional.conv2d(\n",
    "        cur_imgs.mean(2).log().view(1,1,*cur_imgs.shape[:2]),\n",
    "        cur_filter,\n",
    "        padding=tuple((torch.tensor(cur_filter.shape)[2:]-1)/2)\n",
    "    )\n",
    "    \n",
    "    numDivisor = torch.nn.functional.conv2d(\n",
    "        torch.ones_like(cur_imgs.mean(2)).view(1,1,*cur_imgs.shape[:2]),\n",
    "        cur_filter,\n",
    "        padding=tuple((torch.tensor(cur_filter.shape)[2:]-1)/2)\n",
    "    )\n",
    "    \n",
    "    logMeans.div_(numDivisor)\n",
    "    logVars.div_(numDivisor)\n",
    "\n",
    "\n",
    "    imagesc((logVars-logMeans).exp().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want var / mean to be same everywhere (that would mean normalised gain?)\n",
    "# Compute Geometric mean of var/mean:\n",
    "# exp(1/n sum(log(var)-log(mean))) locally, show that corrected image is more stereotypical\n",
    "\n",
    "cur_filter = torch.ones(71,71)\n",
    "cur_filter = cur_filter.div(cur_filter.sum()).view(1,1,*cur_filter.size())\n",
    "\n",
    "xslice = slice(20,490)#slice(0,512)#slice(50,470)#\n",
    "yslice = slice(0,512)#slice(20,490)#slice(20,490)#\n",
    "for cur_imgs in [imgsImputed[yslice,xslice], imgsImputedMeanCorr[yslice,xslice], imgsImputedCorr[yslice,xslice]]:\n",
    "    logVars = torch.nn.functional.conv2d(\n",
    "        cur_imgs.std(2).pow(2).log().view(1,1,*cur_imgs.shape[:2]),\n",
    "        cur_filter,\n",
    "        padding=tuple((torch.tensor(cur_filter.shape)[2:]-1)/2)\n",
    "    )\n",
    "\n",
    "    Means = torch.nn.functional.conv2d(\n",
    "        cur_imgs.mean(2).view(1,1,*cur_imgs.shape[:2]),\n",
    "        cur_filter,\n",
    "        padding=tuple((torch.tensor(cur_filter.shape)[2:]-1)/2)\n",
    "    )\n",
    "    \n",
    "    numDivisor = torch.nn.functional.conv2d(\n",
    "        torch.ones_like(cur_imgs.mean(2)).view(1,1,*cur_imgs.shape[:2]),\n",
    "        cur_filter,\n",
    "        padding=tuple((torch.tensor(cur_filter.shape)[2:]-1)/2)\n",
    "    )\n",
    "    \n",
    "    Means.div_(numDivisor)\n",
    "    logVars.div_(numDivisor)\n",
    "\n",
    "\n",
    "    imagesc(Means.squeeze())\n",
    "    imagesc((logVars).exp().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(numDivisor.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (logVars-logMeans).exp().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(mean_im[slice(40,470),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc((logVars-logMeans).exp().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at the corrected vs original mean-var plots in smaller regions\n",
    "# toUse = imgsImputedCorrected.mean(2)<5000000.\n",
    "# scatterHex(y=imgsImputedCorrected.std(2)[toUse].pow(2.).view(-1).detach().numpy(), \n",
    "#            x=imgsImputedCorrected.mean(2)[toUse].view(-1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at the corrected vs original mean-var plots in smaller regions\n",
    "# toUse = imgsImputed.mean(2)<5000000.\n",
    "# scatterHex(y=imgsImputed.std(2)[toUse].pow(2.).view(-1).detach().numpy(), \n",
    "#            x=imgsImputed.mean(2)[toUse].view(-1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the regions (training data only)\n",
    "import json\n",
    "import numpy as np\n",
    "dims = imgsImputed.shape[:2]\n",
    "\n",
    "with open(data_dir+dataset_name+'/regions/regions.json') as f:\n",
    "    regions = json.load(f)\n",
    "\n",
    "def tomask(coords):\n",
    "    mask = np.zeros(dims)\n",
    "    mask[zip(*coords)] = 1\n",
    "    return mask\n",
    "\n",
    "masks = np.array([tomask(s['coordinates']) for s in regions])\n",
    "\n",
    "masks= torch.tensor(masks.astype(np.float32)).permute(1,2,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesc(masks.sum(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:preprocEnv]",
   "language": "python",
   "name": "conda-env-preprocEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
