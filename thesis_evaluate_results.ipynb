{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate efficacy of gain-correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model\n",
    "import torch\n",
    "import math\n",
    "import gpytorch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "import preprocUtils\n",
    "import preprocRandomVariables\n",
    "import preprocLikelihoods\n",
    "import preprocModels\n",
    "import preprocKernels\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Plotly \n",
    "import plotly\n",
    "from plotly.offline import iplot as plt\n",
    "from plotly import graph_objs as plt_type\n",
    "from plotly import graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import colorcet # For custom colormaps\n",
    "\n",
    "import nbimporter\n",
    "from preprocVisualisationTesting import *\n",
    "from evaluateResults_new import *\n",
    "\n",
    "import time\n",
    "\n",
    "# Get a bunch of useful utility functions for loading data and results\n",
    "from thesis_final_func_defs import *\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/nfs/data/gergo/Neurofinder_update/'\n",
    "\n",
    "\n",
    "stamp_git = '_gitsha_' + '2bd0d720de0995be6b0f1795304839f9877cb6c3'\n",
    "stamp_training_type = '_rPC_1_origPMgain_useNans'\n",
    "stamp_trainingCoverage = '_targetCoverage_10'\n",
    "stamp_modelGridType = '_grid_30_7' \n",
    "\n",
    "remove_PCs = 1\n",
    "\n",
    "final_stamp = stamp_git + stamp_training_type + stamp_trainingCoverage + stamp_modelGridType\n",
    "\n",
    "prior = 'expertPrior'\n",
    "lik = 'unampLik'\n",
    "\n",
    "exportNow = True\n",
    "instant_clear_outputs = exportNow\n",
    "time_stamp = '_20190525T114324'#getTimestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = '3'\n",
    "subdataset = '00'\n",
    "dataset_name = 'neurofinder.0' + data_id + '.' + subdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mll, model, likelihood, train_x, train_y, \\\n",
    "# dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "# display_results(retVars=True,\n",
    "#                 data_id = data_id,\n",
    "#                 subdataset = subdataset,\n",
    "#                 prior=prior, \n",
    "#                 lik=lik,\n",
    "#                 stamp = final_stamp)\n",
    "\n",
    "# imagesc(pred_gain_func, heatmap=dict(colorscale='div'))\n",
    "# imagesc(mean_im, pixels_per_micron=1.15)\n",
    "# imagesc(corr_mean_im, pixels_per_micron=1.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'\n",
    "\n",
    "# Load the appropriate fitted model\n",
    "mll, model, likelihood, train_x, train_y, \\\n",
    "dataStats, mean_im, pred_gain_func, corr_mean_im = \\\n",
    "loadFittedModel(\n",
    "    dataset_name = dataset_name,\n",
    "    data_dir=data_dir,\n",
    "    prior=prior, \n",
    "    lik=lik, \n",
    "    stamp = final_stamp,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "# Load also the corresponding linear likelihood model!\n",
    "lin_model_list = \\\n",
    "loadFittedModel(\n",
    "    dataset_name = dataset_name,\n",
    "    data_dir=data_dir,\n",
    "    prior=prior, \n",
    "    lik='linLik', \n",
    "    stamp = final_stamp,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "#\n",
    "lin_likelihood = lin_model_list[0].likelihood\n",
    "lin_likParams = OrderedDict(lin_likelihood.named_parameters())\n",
    "\n",
    "# Store the expected additive \"electronic\" noise (based on this linear model estimate)\n",
    "gauss_noise_std = lin_likParams['log_noise'].data[0].exp().sqrt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the appropriate data (with potentially correcting for photomultiple gain included in the mll model object\n",
    "imgsImputed = loadImputedData(\n",
    "    dataset_name = dataset_name,\n",
    "    data_dir=data_dir,\n",
    "    device = device,\n",
    "    # We can supply a model that corrects for the photomultipler gain\n",
    "    mll = mll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likParams = OrderedDict(mll.likelihood.named_parameters())\n",
    "# for key in ['log_gain', 'log_underamplified_amplitude']:\n",
    "#     print(key, np.exp(likParams[key].data[0]))\n",
    "    \n",
    "# for key in ['log_noise', 'log_noise_pedestal']:\n",
    "#     print(key, np.sqrt(np.exp(likParams[key].data[0])))\n",
    "print('offset', likParams['offset'].data[0])\n",
    "print('log_noise_pedestal', np.sqrt(np.exp(likParams['log_noise_pedestal'].data[0])))\n",
    "print('logit_underamplified_probability', logistic(likParams['logit_underamplified_probability'].data[0]))\n",
    "print('log_underamplified_amplitude', np.exp(likParams['log_underamplified_amplitude'].data[0]))\n",
    "print('log_gain', np.exp(likParams['log_gain'].data[0]))\n",
    "print('log_noise', np.sqrt(np.exp(likParams['log_noise'].data[0])))\n",
    "print('max pixel value', imgsImputed.max())\n",
    "print('max photon count', (imgsImputed.max().cpu()-likParams['offset'].data[0].cpu())/np.exp(likParams['log_gain'].data[0].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_PCs is not None:\n",
    "    # # Get the PCs from full dataset\n",
    "    U, S, V = torch.svd((imgsImputed-imgsImputed.mean(2).unsqueeze(-1)).view(-1, imgsImputed.size(2)).cpu())\n",
    "\n",
    "    # Get the PCs from just training data (this seems to be a bad option for locating pixels based on crossCorr)\n",
    "    # U, S, V = torch.svd((train_y-train_y.mean(1).unsqueeze(-1)))\n",
    "\n",
    "    to_remove = U[:, :remove_PCs].matmul(S[:remove_PCs].diag()).matmul(V[:,:remove_PCs].t())\n",
    "\n",
    "    imgsImputed -= to_remove.view(*imgsImputed.size()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MAP transformation \n",
    "gray_levels, inverse_poiss_MAP = getInverseMapEstimate(\n",
    "    likelihood,\n",
    "    max_gray_level = preprocUtils.nanmax(imgsImputed.cpu()),\n",
    "    max_photon = float(200)\n",
    ")\n",
    "gray_levels = gray_levels[2:] # Ignore negative values\n",
    "inverse_poiss_MAP = inverse_poiss_MAP[2:] # Ignore negative values\n",
    "\n",
    "\n",
    "\n",
    "# Get the MAP transformation for the linear model\n",
    "lin_gray_levels, lin_inverse_poiss_MAP = getInverseMapEstimate(\n",
    "    lin_likelihood,\n",
    "    max_gray_level = preprocUtils.nanmax(imgsImputed.cpu()),\n",
    "    max_photon = float(70)\n",
    ")\n",
    "lin_gray_levels = lin_gray_levels[2:] # Ignore negative values\n",
    "lin_inverse_poiss_MAP = lin_inverse_poiss_MAP[2:] # Ignore negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsImputedLin = imgsImputed.cpu().detach() - lin_likelihood.offset.data[0].cpu().detach()\n",
    "imgsImputedLin[imgsImputedLin < 0] = 0.\n",
    "\n",
    "# For the linear case we have to know that the spatial and likelihood gain function have a non-determinancy, \n",
    "# so assume that the spatial mean gain was 1, and normalise the dataset accordingly with this now effective total gain\n",
    "lin_pred_gain_func = lin_model_list[-2]\n",
    "lin_mean_spatial_gain_train = lin_pred_gain_func[train_x.detach().long().unbind(1)].mean().detach()\n",
    "lin_pred_gain_func = lin_pred_gain_func.div(lin_mean_spatial_gain_train)\n",
    "\n",
    "imgsImputedLin = imgsImputedLin.div(\n",
    "    lin_likelihood.log_gain.data[0].exp().cpu() * lin_mean_spatial_gain_train.cpu()\n",
    ").detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsImputedPhoton = torch.stack([\n",
    "    progress_bar(\n",
    "        func = lambda image: im2photon(image, inverse_poiss_MAP, gray_levels, keep_zeros=True).to('cpu').detach(),\n",
    "        inp = image,\n",
    "        index = index,\n",
    "        report = True,\n",
    "        report_freq = 400\n",
    "    )\n",
    "    for index, image in enumerate(imgsImputed.permute(2,0,1).detach().to(device))], \n",
    "    dim=2).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the individual images with the gain\n",
    "# gainRange = [1./100, 100.]\n",
    "# imgsImputedCorr = ((imgsImputedPhoton)\n",
    "#                    .div(torch.clamp(pred_gain_func.to('cpu'), min=gainRange[0], max=gainRange[1]).unsqueeze(2))\n",
    "#                   )\n",
    "\n",
    "imgsImputedCorr = imgsImputedPhoton.div(pred_gain_func.to('cpu').unsqueeze(2)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Nan dataset, to make sure detected Nan pixels do not form part of further analyses\n",
    "imgsNan = loadNanData(\n",
    "    dataset_name = dataset_name,\n",
    "    data_dir=data_dir,\n",
    "    device = 'cpu',\n",
    "    # We can supply a model that corrects for the photomultipler gain\n",
    "    mll = mll\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsImputed = imgsImputed.cpu()\n",
    "train_x = train_x.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanmask = torch.isnan(imgsNan[:,:,:imgsImputed.shape[2]])\n",
    "imgsImputed[nanmask] = float('nan')\n",
    "imgsImputedLin[nanmask] = float('nan')\n",
    "imgsImputedPhoton[nanmask] = float('nan')\n",
    "imgsImputedCorr[nanmask] = float('nan')\n",
    "\n",
    "#imagesc(preprocUtils.nanmean(imgsImputedCorr.detach(), dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsImputedLin = imgsImputedLin.detach()\n",
    "imgsImputedPhoton = imgsImputedPhoton.detach()\n",
    "imgsImputedCorr = imgsImputedCorr.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate likelihood transform (gray -> photon)\n",
    "\n",
    "\n",
    "Evaluates the affine fit to mean-variance description of the pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise(\"Stop execute all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_skip = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, plot the resulting non-linear maximum likelihood transformation curve\n",
    "#data = plot(X=gray_levels, Y=inverse_poiss_MAP, now = False)\n",
    "\n",
    "# Using the Gaussian noise estimate from the linear model\n",
    "\n",
    "data = [\n",
    "    plt_type.Scatter( x = lin_gray_levels.view(-1)-2*gauss_noise_std, y=lin_inverse_poiss_MAP.view(-1),\n",
    "                     line=dict(color='rgba(0,0,0,0.2)')\n",
    "                    ),\n",
    "    plt_type.Scatter( x = lin_gray_levels.view(-1), y=lin_inverse_poiss_MAP.view(-1),\n",
    "                fill='tonextx',\n",
    "                fillcolor='rgba(0,0,0,0.2)'\n",
    "                    ),\n",
    "    plt_type.Scatter( x = lin_gray_levels.view(-1)+2*gauss_noise_std, y=lin_inverse_poiss_MAP.view(-1),\n",
    "                fill='tonextx',\n",
    "                line=dict(color='rgba(0,0,0,0.2)'),\n",
    "                fillcolor='rgba(0,0,0,0.2)'\n",
    "                    )\n",
    "]\n",
    "\n",
    "layoutArgs = defaultLayout(scale=1.2)\n",
    "dict_merge(layoutArgs, dict(\n",
    "    xaxis = dict(\n",
    "        title = 'Grey level in data (a.u.)',\n",
    "        range = [0, gray_levels.max()]\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'Estimated photon flux (photon)',\n",
    "        showgrid=False\n",
    "    ),\n",
    "    colorway = ['black'],\n",
    "    showlegend = False\n",
    "))\n",
    "\n",
    "fig = plt_type.Figure(data=data, layout=plt_type.Layout(**layoutArgs))\n",
    "\n",
    "if exportNow:\n",
    "    exportFigure(fig, filename= 'ch1_figResults_InverseML_Lin_nf0'+ data_id + subdataset, \n",
    "                 time_stamp = time_stamp, type='plot', image='svg')\n",
    "else:\n",
    "    plt(fig)\n",
    "\n",
    "if instant_clear_outputs:\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear photomultiplier likelihood transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import thesis_final_func_defs\n",
    "# importlib.reload(thesis_final_func_defs)\n",
    "# from thesis_final_func_defs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the marginal probability distribution over incoming photon numbers at each observed grey level\n",
    "\n",
    "# Generate marginal photon count distributions at various input levels ()\n",
    "max_input = 20.\n",
    "photon_counts, photon_log_probs = (\n",
    "    likelihood.getPhotonLogProbs(torch.arange(0.,max_input).view(1,-1).to(gray_levels.device)*torch.ones_like(gray_levels).view(-1,1), \n",
    "                                 max_photon=float(200), reNormalise = False))\n",
    "\n",
    "# Get the response distribution at each potential photon count\n",
    "p_PM = likelihood.createResponseDistributions(photon_counts) \n",
    "\n",
    "# Get the marginal log probabilities of each observation at each photon count\n",
    "cur_target_slice = gray_levels.view(-1)\n",
    "allLogProbs = torch.cat(\n",
    "                [p_PM[0].log_prob(cur_target_slice.view(-1,1)).view(-1,1),\n",
    "                p_PM[1].log_prob(cur_target_slice.view(-1,1)).view(-1,1),\n",
    "                p_PM[2].log_prob(cur_target_slice.view(-1,1))],\n",
    "                dim = 1)\n",
    "            \n",
    "# Correct for the less than 1 observations with log CDF instead of log_prob\n",
    "if (cur_target_slice<=0.).sum()>0:       \n",
    "    allLogProbs[cur_target_slice<=0., :] = torch.cat(\n",
    "        [p_PM[0].cdf(0.).log().view(-1),\n",
    "        p_PM[1].cdf(0.).log().view(-1),\n",
    "        p_PM[2].cdf(0.).log().view(-1)],\n",
    "        dim = 0)\n",
    "    \n",
    "\n",
    "# Create the figure\n",
    "allProbs = allLogProbs.exp().div(allLogProbs.exp().sum(1).view(-1,1)).detach().data\n",
    "n_max = 11\n",
    "\n",
    "layout_extra = defaultLayout(scale=1.2)\n",
    "dict_merge(layout_extra, dict(xaxis=dict(title='Grey level in data (a.u.)')))\n",
    "\n",
    "fig = plotStacked(\n",
    "    torch.cat([allProbs[:,:n_max], (1.-allProbs[:,:n_max].sum(1)).view(-1,1)],dim=1), \n",
    "    gray_levels, \n",
    "    now=False,\n",
    "    layout=layout_extra\n",
    ")\n",
    "\n",
    "\n",
    "if exportNow:\n",
    "    exportFigure(fig, filename= 'ch1_figResults_InverseMarginals_nf0'+ data_id + subdataset, \n",
    "                 time_stamp = time_stamp, type='plot', image='svg')\n",
    "else:\n",
    "    plt(fig)\n",
    " \n",
    "\n",
    "if instant_clear_outputs:\n",
    "    clear_output()\n",
    "    time.sleep(5) # Put sleep statements so browser can clear buffer and doesn't download empty plots\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_level_stds = im2photon(\n",
    "    torch.stack([gray_levels-gauss_noise_std, gray_levels+gauss_noise_std], dim=1), \n",
    "                inverse_poiss_MAP, gray_levels, keep_zeros=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, plot the resulting non-linear maximum likelihood transformation curve\n",
    "#data = plot(X=gray_levels, Y=inverse_poiss_MAP, now = False)\n",
    "\n",
    "# Get the error bars in photon flux estimates, by squashing the \n",
    "# Gaussian distribution mean-std, mean+std curves (the assumed \"electronic noise\")\n",
    "# through the estimated non-linear transform induced by the photomultiplier\n",
    "gray_level_stds = im2photon(\n",
    "    torch.stack([gray_levels-gauss_noise_std, gray_levels+gauss_noise_std], dim=1), \n",
    "                inverse_poiss_MAP, gray_levels, keep_zeros=False)\n",
    "\n",
    "data = [\n",
    "    plt_type.Scatter( x = gray_levels.view(-1), y=gray_level_stds[:,0],\n",
    "                     line=dict(color='rgba(0,0,0,0.2)')\n",
    "                    ),\n",
    "    plt_type.Scatter( x = gray_levels.view(-1), y=inverse_poiss_MAP.view(-1),\n",
    "                fill='tonextx',\n",
    "                fillcolor='rgba(0,0,0,0.2)'\n",
    "                    ),\n",
    "    plt_type.Scatter( x = gray_levels.view(-1), y=gray_level_stds[:,1],\n",
    "                fill='tonextx',\n",
    "                line=dict(color='rgba(0,0,0,0.2)'),\n",
    "                fillcolor='rgba(0,0,0,0.2)'\n",
    "                    )\n",
    "]\n",
    "\n",
    "layoutArgs = defaultLayout(scale=1.2)\n",
    "dict_merge(layoutArgs, dict(\n",
    "    xaxis = dict(\n",
    "        title = 'Grey level in data (a.u.)',\n",
    "        range = [0, gray_levels.max()]\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'Estimated photon flux (photon)',\n",
    "        showgrid=False\n",
    "    ),\n",
    "    colorway = ['black'],\n",
    "    showlegend = False\n",
    "))\n",
    "\n",
    "fig = plt_type.Figure(data=data, layout=plt_type.Layout(**layoutArgs))\n",
    "\n",
    "if exportNow:\n",
    "    exportFigure(fig, filename= 'ch1_figResults_InverseML_NonLin_nf0'+ data_id + subdataset, \n",
    "                 time_stamp = time_stamp, type='plot', image='svg')\n",
    "else:\n",
    "    plt(fig)\n",
    "\n",
    "if instant_clear_outputs:\n",
    "    clear_output()\n",
    "    time.sleep(5) # Put sleep statements so browser can clear buffer and doesn't download empty plots\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the forward model estimates\n",
    "# inp = gpytorch.random_variables.GaussianRandomVariable(inverse_poiss_MAP.view(-1), \n",
    "#                              gpytorch.lazy.DiagLazyVariable(1e-10*torch.ones_like(inverse_poiss_MAP.view(-1)))\n",
    "#                             )\n",
    "# pred_at_inverseMAP = likelihood(latent_func = inp, approx = False, max_photon=int(570))\n",
    "# pred_at_inverseMAP_mean = pred_at_inverseMAP.mean()\n",
    "# pred_at_inverseMAP_mean\n",
    "\n",
    "# plot(\n",
    "#     Y = torch.stack(\n",
    "#         [pred_at_inverseMAP.mean()-pred_at_inverseMAP.std(),\n",
    "#         pred_at_inverseMAP.mean(), \n",
    "#          pred_at_inverseMAP.mean()+pred_at_inverseMAP.std()],\n",
    "#         dim = 1\n",
    "#     ),\n",
    "#     X =  inverse_poiss_MAP\n",
    "#     )\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(\n",
    "#     X = torch.stack(\n",
    "#         [gray_levels-2*gauss_noise_std,\n",
    "#         gray_levels, \n",
    "#          gray_levels+2*gauss_noise_std],\n",
    "#         dim = 1\n",
    "#     ), \n",
    "#     Y=inverse_poiss_MAP.view(-1,1).repeat((1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(\n",
    "#     Y = torch.stack(\n",
    "#         [pred_at_inverseMAP_mean-pred_at_inverseMAP_std/2,\n",
    "#         pred_at_inverseMAP_mean, \n",
    "#          pred_at_inverseMAP_mean+pred_at_inverseMAP_std/2],\n",
    "#         dim = 1\n",
    "#     ),\n",
    "#     X =  inverse_poiss_MAP.view(-1)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Get the forward model estimates\n",
    "# inp = gpytorch.random_variables.GaussianRandomVariable(inverse_poiss_MAP.view(-1).log(), \n",
    "#                              gpytorch.lazy.DiagLazyVariable(1e-10*torch.ones_like(inverse_poiss_MAP.view(-1)))\n",
    "#                             )\n",
    "# pred_at_inverseMAP = likelihood(latent_func = inp, approx = False, max_photon=int(70))\n",
    "# pred_at_inverseMAP_mean = pred_at_inverseMAP.mean()\n",
    "# pred_at_inverseMAP_std = pred_at_inverseMAP.std()\n",
    "\n",
    "# # plot(\n",
    "# #     X = torch.stack(\n",
    "# #         [gray_levels-,\n",
    "# #         gray_levels, \n",
    "# #          gray_levels+pred_at_inverseMAP.std()],\n",
    "# #         dim = 1\n",
    "# #     ),\n",
    "# #     Y =  inverse_poiss_MAP.view(-1,1).repeat(1,3)\n",
    "# #     )\n",
    "\n",
    "# plot(\n",
    "#     X = torch.stack(\n",
    "#         [pred_at_inverseMAP_mean-pred_at_inverseMAP_std/2,\n",
    "#         gray_levels,\n",
    "#          pred_at_inverseMAP_mean,\n",
    "#          pred_at_inverseMAP_mean+pred_at_inverseMAP_std/2],\n",
    "#         dim = 1\n",
    "#     ),\n",
    "#     Y =  inverse_poiss_MAP.view(-1,1).repeat(1,4)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Also, plot the resulting non-linear maximum likelihood transformation curve\n",
    "# data = plot(X=gray_levels, Y=inverse_poiss_MAP, now = False)\n",
    "# layoutArgs = defaultLayout(scale=1.2)\n",
    "# dict_merge(layoutArgs, dict(\n",
    "#     xaxis = dict(\n",
    "#         title = 'Grey level in data (a.u.)'\n",
    "#     ),\n",
    "#     yaxis = dict(\n",
    "#         title = 'Estimated photon flux (photon)',\n",
    "#         showgrid=False\n",
    "#     ),\n",
    "#     colorway = ['black']\n",
    "# ))\n",
    "\n",
    "# fig = plt_type.Figure(data=data, layout=plt_type.Layout(**layoutArgs))\n",
    "\n",
    "# if exportNow:\n",
    "#     exportFigure(fig, filename= 'ch1_figResults_InverseML_NonLin_nf0'+ data_id + subdataset, \n",
    "#                  time_stamp = time_stamp, type='plot', image='svg')\n",
    "# else:\n",
    "#     plt(fig)\n",
    "\n",
    "# if instant_clear_outputs:\n",
    "#     clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric effects of the transforms on mean-variance plots and relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variance over mean curves to test the likelihood estimating the correct gain\n",
    "\n",
    "mean_orig = preprocUtils.nanmean(imgsImputed[train_x.long().unbind(1)][:,1000:].contiguous(), -1)\n",
    "var_orig = preprocUtils.nanvar(imgsImputed[train_x.long().unbind(1)][:,1000:].contiguous(), -1)\n",
    "mean_orig = mean_orig[torch.isnan(mean_orig)==0]\n",
    "var_orig = var_orig[torch.isnan(var_orig)==0]\n",
    "\n",
    "mean_lin = preprocUtils.nanmean(imgsImputedLin[train_x.long().unbind(1)][:,1000:].contiguous(), -1)\n",
    "var_lin = preprocUtils.nanvar(imgsImputedLin[train_x.long().unbind(1)][:,1000:].contiguous(), -1)\n",
    "mean_lin = mean_lin[torch.isnan(mean_lin)==0]\n",
    "var_lin = var_lin[torch.isnan(var_lin)==0]\n",
    "\n",
    "mean_photon = preprocUtils.nanmean(imgsImputedPhoton[train_x.long().unbind(1)][:,1000:].contiguous(), -1)\n",
    "var_photon = preprocUtils.nanvar(imgsImputedPhoton[train_x.long().unbind(1)][:,1000:].contiguous(), -1)\n",
    "mean_photon = mean_photon[torch.isnan(mean_photon)==0]\n",
    "var_photon = var_photon[torch.isnan(var_photon)==0]\n",
    "\n",
    "mean_corr = preprocUtils.nanmean(imgsImputedCorr[train_x.long().unbind(1)][:,1000:].contiguous(), -1)\n",
    "var_corr = preprocUtils.nanvar(imgsImputedCorr[train_x.long().unbind(1)][:,1000:].contiguous(), -1)\n",
    "mean_corr = mean_corr[torch.isnan(mean_corr)==0]\n",
    "var_corr = var_corr[torch.isnan(var_corr)==0]\n",
    "\n",
    "affine_orig_fit = preprocUtils.torchLinReg(mean_orig.view(-1,1).cpu(), var_orig.view(-1,1).cpu(), exact=True)\n",
    "affine_lin_fit = preprocUtils.torchLinReg(mean_lin.view(-1,1), var_lin.view(-1,1), exact=True)\n",
    "affine_photon_fit = preprocUtils.torchLinReg(mean_photon.view(-1,1), var_photon.view(-1,1), exact=True)\n",
    "affine_corr_fit = preprocUtils.torchLinReg(mean_corr.view(-1,1), var_corr.view(-1,1), exact=True)\n",
    "print(affine_orig_fit[2], affine_orig_fit[0])\n",
    "print(affine_lin_fit[2], affine_lin_fit[0])\n",
    "print(affine_photon_fit[2], affine_photon_fit[0])\n",
    "print(affine_corr_fit[2], affine_corr_fit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all plots' information to iterate over\n",
    "all_likelihood_gain_plots = {\n",
    "    'orig': {\n",
    "        'mean':mean_orig[::vis_skip],\n",
    "        'var':var_orig[::vis_skip],\n",
    "        'fit':affine_orig_fit,\n",
    "        'fname':'ch1_figResults_GainEstmation_Orig_nf0',\n",
    "        'title':'Original observations'\n",
    "    },\n",
    "    'lin': {\n",
    "        'mean':mean_lin[::vis_skip],\n",
    "        'var':var_lin[::vis_skip],\n",
    "        'fit':affine_lin_fit,\n",
    "        'fname':'ch1_figResults_GainEstmation_Lin_nf0',\n",
    "        'title':'Linear likelihood gain correction'\n",
    "    },\n",
    "    'photon': {\n",
    "        'mean':mean_photon[::vis_skip],\n",
    "        'var':var_photon[::vis_skip],\n",
    "        'fit':affine_photon_fit,\n",
    "        'fname':'ch1_figResults_GainEstmation_Photon_nf0',\n",
    "        'title':'Photomultiplier likelihood gain correction'\n",
    "    },\n",
    "    'corr': {\n",
    "        'mean':mean_corr[::vis_skip],\n",
    "        'var':var_corr[::vis_skip],\n",
    "        'fit':affine_corr_fit,\n",
    "        'fname':'ch1_figResults_GainEstmation_Corr_nf0',\n",
    "        'title':'Photomultiplier likelihood and spatial gain correction'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all plots in a loop\n",
    "for key in all_likelihood_gain_plots:#['photon']:#\n",
    "\n",
    "    predictor = all_likelihood_gain_plots[key]['mean']\n",
    "    target = all_likelihood_gain_plots[key]['var']\n",
    "\n",
    "    cur_affine_fit = all_likelihood_gain_plots[key]['fit']\n",
    "    y_gain_linear = cur_affine_fit[0]\n",
    "    y_gain_intercept = cur_affine_fit[1]\n",
    "    y_gain_offset = cur_affine_fit[2]\n",
    "\n",
    "    pred_at = torch.linspace(0, predictor.max(),100)\n",
    "    pred_result = (y_gain_linear*pred_at+y_gain_intercept).squeeze()\n",
    "    n_outliers = (target>pred_result.max()).sum()\n",
    "\n",
    "\n",
    "    layoutArgs = defaultLayout(scale=1.2)\n",
    "    dict_merge(layoutArgs, dict(\n",
    "        xaxis = dict(\n",
    "            title = 'Mean intensity over time (a.u.)'\n",
    "        ),\n",
    "        yaxis = dict(\n",
    "            title = 'Variance over time (a.u.<sup>2</sup>)',\n",
    "            showgrid=True,\n",
    "            range = [min([pred_result.min(), predictor.min()]), pred_result.max()]\n",
    "        ),\n",
    "        title = all_likelihood_gain_plots[key]['title'],\n",
    "\n",
    "        annotations=[        \n",
    "            dict(\n",
    "                x=0.25,\n",
    "                y=0.9,\n",
    "                xanchor = 'center',\n",
    "                yanchor='top',\n",
    "                xref='paper',\n",
    "                yref='paper',\n",
    "                text='{} outliers / {} data'.format(int(n_outliers), target.numel()),\n",
    "                showarrow=False,\n",
    "                arrowhead=0,\n",
    "                ax=0,\n",
    "                ay=0\n",
    "            ),\n",
    "            dict(\n",
    "                x=0.65,\n",
    "                y=0.3,\n",
    "                xanchor = 'center',\n",
    "                yanchor='top',\n",
    "                xref='paper',\n",
    "                yref='paper',\n",
    "                text='Affine fit: y = {:0.2f} * (x-{:0.2f})'.format(float(y_gain_linear.data[0]), float(y_gain_offset.data[0])),\n",
    "                font = dict(color='#CC8C00'),\n",
    "                showarrow=False,\n",
    "                arrowhead=0,\n",
    "                ax=0,\n",
    "                ay=0\n",
    "            )   \n",
    "        ]\n",
    "    ))\n",
    "\n",
    "    fig= plt_type.Figure(\n",
    "        data = [plt_type.Scattergl(x=predictor, y=target, \n",
    "                      mode='markers', \n",
    "                      marker=dict(\n",
    "                          opacity=0.3,\n",
    "                          color='black',\n",
    "                          size=4\n",
    "                      ),\n",
    "                      name='Background pixels'\n",
    "                     ),\n",
    "        plt_type.Scattergl(x=pred_at, y=pred_result, \n",
    "                     marker=dict(color='orange'),\n",
    "                     name='Affine fit'\n",
    "                    )\n",
    "        ],\n",
    "        layout = plt_type.Layout(**layoutArgs)\n",
    "    )\n",
    "\n",
    "    if exportNow:\n",
    "        exportFigure(fig, filename= all_likelihood_gain_plots[key]['fname']+ data_id + subdataset, \n",
    "                     time_stamp = time_stamp, type='plot', image='svg')\n",
    "    else:\n",
    "        plt(fig)\n",
    "\n",
    "    if instant_clear_outputs:\n",
    "        clear_output()\n",
    "        time.sleep(5) # Put sleep statements so browser can clear buffer and doesn't download empty plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial gain correction\n",
    "\n",
    "Show images of the spatial gain correction procedure\n",
    "\n",
    "Get for each training pixel the optimal Lambda (photon flux)\n",
    "\n",
    "Compare the histograms on the original versus the corrected values as numerical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pixel_per_micron = {'0':1.15, '1':0.8, '2':1.15, '3':1.7, '4':0.8}\n",
    "pixels_per_micron = all_pixel_per_micron[data_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagesc(pred_gain_func, heatmap=dict(colorscale='div'), pixels_per_micron=pixels_per_micron, image='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = imagesc(pred_gain_func, heatmap=dict(colorscale='div'), pixels_per_micron=pixels_per_micron, now=False)\n",
    "\n",
    "if exportNow:\n",
    "    exportFigure(fig, filename= 'ch1_figResults_SpatialGainMap_nf0'+ data_id + subdataset, \n",
    "                 time_stamp = time_stamp, type='image', image='svg')\n",
    "else:\n",
    "    plt(fig)\n",
    "\n",
    "if instant_clear_outputs:\n",
    "    clear_output()\n",
    "    time.sleep(5) # Put sleep statements so browser can clear buffer and doesn't download empty plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagesc(preprocUtils.nanmean(imgsImputed[:,:,1000:].detach(), dim=2), pixels_per_micron=pixels_per_micron, image='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = imagesc(preprocUtils.nanmean(imgsImputed[:,:,1000:].detach(), dim=2), pixels_per_micron=pixels_per_micron, now=False)\n",
    "\n",
    "if exportNow:\n",
    "    exportFigure(fig, filename= 'ch1_figResults_SpatialOrigMean_nf0'+ data_id + subdataset, \n",
    "                 time_stamp = time_stamp, type='image', image='svg')\n",
    "else:\n",
    "    plt(fig)\n",
    "    \n",
    "if instant_clear_outputs:\n",
    "    clear_output()\n",
    "    time.sleep(5) # Put sleep statements so browser can clear buffer and doesn't download empty plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagesc(preprocUtils.nanmean(imgsImputedPhoton[:,:,1000:].detach(), dim=2), pixels_per_micron=pixels_per_micron, image='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = imagesc(preprocUtils.nanmean(imgsImputedPhoton[:,:,1000:].detach(), dim=2), pixels_per_micron=pixels_per_micron, now=False)\n",
    "\n",
    "if exportNow:\n",
    "    exportFigure(fig, filename= 'ch1_figResults_SpatialPhotonMean_nf0'+ data_id + subdataset, \n",
    "                 time_stamp = time_stamp, type='image', image='svg')\n",
    "else:\n",
    "    plt(fig)\n",
    "    \n",
    "if instant_clear_outputs:\n",
    "    clear_output()\n",
    "    time.sleep(5) # Put sleep statements so browser can clear buffer and doesn't download empty plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagesc(preprocUtils.nanmean(imgsImputedCorr[:,:,1000:].detach(), dim=2), pixels_per_micron=pixels_per_micron, image='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = imagesc(preprocUtils.nanmean(imgsImputedCorr[:,:,1000:].detach(), dim=2), pixels_per_micron=pixels_per_micron, now=False)\n",
    "\n",
    "if exportNow:\n",
    "    exportFigure(fig, filename= 'ch1_figResults_SpatialCorrMean_nf0'+ data_id + subdataset, \n",
    "                 time_stamp = time_stamp, type='image', image='svg')\n",
    "else:\n",
    "    plt(fig)\n",
    "    \n",
    "if instant_clear_outputs:\n",
    "    clear_output()\n",
    "    time.sleep(5) # Put sleep statements so browser can clear buffer and doesn't download empty plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise('Stopping Run All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the single Lambda rate per pixel (over all time samples) and show histograms for correcting that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "max_input = 20.\n",
    "photon_counts, photon_log_probs = (\n",
    "    likelihood.getPhotonLogProbs(torch.arange(0.,max_input).view(1,-1).to(gray_levels.device)*torch.ones_like(gray_levels).view(-1,1), \n",
    "                                 max_photon=float(200), reNormalise = False))\n",
    "\n",
    "# Get the response distribution at each potential photon count\n",
    "p_PM = likelihood.createResponseDistributions(photon_counts) \n",
    "photon_log_probs_gray = likelihood.getLogProbSumOverTargetSamples(p_PM, gray_levels.view(-1))\n",
    "\n",
    "#orig_dataset = TensorDataset(imgsImputed.cuda())\n",
    "test_dataset = TensorDataset(imgsImputed[train_x.long().unbind(1)][:,1000:].contiguous().to(device))\n",
    "data_loader = DataLoader(test_dataset, batch_size=50, shuffle=False, drop_last=False)\n",
    "\n",
    "out = []\n",
    "for mini_batch in data_loader:\n",
    "    log_w = im2logPhotonProb(mini_batch[0], photon_log_probs_gray.to(device), gray_levels.to(device))\n",
    "    out.append(\n",
    "        getOptLambda(\n",
    "            log_w,\n",
    "            lambda_guess = log_w.max(-1)[1].float().mean(-1)\n",
    "        ).cpu()\n",
    "    )\n",
    "    \n",
    "testLambda = torch.cat(out, dim=0).squeeze().detach()\n",
    "testLambdaCorr = testLambda.div(pred_gain_func[train_x.long().unbind(1)].view(-1).cpu()).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max_vals = [float(preprocUtils.nanmax(testLambda)), float(preprocUtils.nanmax(testLambdaCorr))]\n",
    "data_max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get histograms of the gain-corrected versus non-corrected photon counts to test gain estimation\n",
    "\n",
    "r0 = 0.\n",
    "#r1 = np.floor(float(imgsImputed.max()/2))\n",
    "r1 = float(max(data_max_vals)) # Use the original range\n",
    "nbins = 100\n",
    "\n",
    "Ahists, bin_centers = fast_histograms(testLambda.view(1,-1).to(device), r0, r1, nbins, output_device='cpu')\n",
    "Ahists_corr, bin_centers = fast_histograms(testLambdaCorr.view(1,-1).to(device), r0, r1, nbins, output_device='cpu')\n",
    "\n",
    "hists = torch.stack([Ahists.detach().view(-1), Ahists_corr.detach().view(-1)], dim=1).detach()\n",
    "\n",
    "#plot(hists.log(), bin_centers.view(-1).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layoutArgs = defaultLayout()\n",
    "dict_merge(layoutArgs, dict(\n",
    "    xaxis=dict(\n",
    "        title = 'Mean Inferred Background Photon flux (per pixel, across frames)'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title = 'Counts per bin',\n",
    "        #type='log'\n",
    "    ),\n",
    "    barmode='overlay'\n",
    "    \n",
    "))\n",
    "\n",
    "first_bin_cent = torch.tensor([bin_centers[0]-bin_centers[1]])\n",
    "right_bin_width = 0.2\n",
    "\n",
    "fig = plt_type.Figure(\n",
    "        data = [\n",
    "            plt_type.Bar(\n",
    "                x=torch.cat([first_bin_cent, bin_centers[1:-1], torch.tensor([bin_centers[-1]+right_bin_width/2.])]),\n",
    "                y=hists[:,0], \n",
    "                width = torch.cat([bin_centers[1]-first_bin_cent, bin_centers[2:]-bin_centers[1:-1],torch.tensor([right_bin_width])]),\n",
    "                marker=dict(\n",
    "                          opacity=0.7,\n",
    "                          color='black',\n",
    "                          #size=4\n",
    "                      ),\n",
    "                name = 'Original'\n",
    "            ),\n",
    "            plt_type.Bar(\n",
    "                x=torch.cat([first_bin_cent, bin_centers[1:-1], torch.tensor([bin_centers[-1]+right_bin_width/2.])]), \n",
    "                y=hists[:,1], \n",
    "                width = torch.cat([bin_centers[1]-first_bin_cent, bin_centers[2:]-bin_centers[1:-1],torch.tensor([right_bin_width])]),\n",
    "                marker=dict(\n",
    "                          opacity=0.3,\n",
    "                          color='orange',\n",
    "                          #size=4\n",
    "                      ),\n",
    "                name = 'Gain Corrected'\n",
    "            ),\n",
    "        ],\n",
    "        layout = plt_type.Layout(**layoutArgs)\n",
    "    )\n",
    "\n",
    "if exportNow:\n",
    "    exportFigure(fig, filename= 'ch1_figResultsCorrHistogram_nf0'+ data_id + subdataset, \n",
    "                 time_stamp = time_stamp,\n",
    "                 type='plot', image='svg')\n",
    "else:\n",
    "    plt(fig)\n",
    "    \n",
    "if instant_clear_outputs:\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLambdaCorr.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALSO DO IT BY MATCHING MEANS OF THE DISTRIBUTIONS\n",
    "# Get histograms of the gain-corrected versus non-corrected photon counts to test gain estimation\n",
    "\n",
    "div_corr = preprocUtils.nanmean(testLambdaCorr)/preprocUtils.nanmean(testLambda)\n",
    "data_max_vals[1] = data_max_vals[1]/div_corr\n",
    "\n",
    "r0 = 0.\n",
    "#r1 = np.floor(float(imgsImputed.max()/2))\n",
    "r1 = float(max(data_max_vals)) # Use the original range\n",
    "nbins = 100\n",
    "\n",
    "\n",
    "Ahists, bin_centers = fast_histograms(testLambda.view(1,-1).to(device), r0, r1, nbins, output_device='cpu')\n",
    "Ahists_corr, bin_centers = fast_histograms(testLambdaCorr.view(1,-1).div(div_corr).to(device), r0, r1, nbins, output_device='cpu')\n",
    "\n",
    "hists = torch.stack([Ahists.detach().view(-1), Ahists_corr.detach().view(-1)], dim=1).detach()\n",
    "\n",
    "#plot(hists.log(), bin_centers.view(-1).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layoutArgs = defaultLayout()\n",
    "dict_merge(layoutArgs, dict(\n",
    "    xaxis=dict(\n",
    "        title = 'Mean Inferred Background Photon flux (per pixel, across frames)'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title = 'Counts per bin',\n",
    "        #type='log'\n",
    "    ),\n",
    "    barmode='overlay'\n",
    "    \n",
    "))\n",
    "\n",
    "first_bin_cent = torch.tensor([bin_centers[0]-bin_centers[1]])\n",
    "right_bin_width = 0.2\n",
    "\n",
    "fig = plt_type.Figure(\n",
    "        data = [\n",
    "            plt_type.Bar(\n",
    "                x=torch.cat([first_bin_cent, bin_centers[1:-1], torch.tensor([bin_centers[-1]+right_bin_width/2.])]),\n",
    "                y=hists[:,0], \n",
    "                width = torch.cat([bin_centers[1]-first_bin_cent, bin_centers[2:]-bin_centers[1:-1],torch.tensor([right_bin_width])]),\n",
    "                marker=dict(\n",
    "                          opacity=0.7,\n",
    "                          color='black',\n",
    "                          #size=4\n",
    "                      ),\n",
    "                name = 'Original'\n",
    "            ),\n",
    "            plt_type.Bar(\n",
    "                x=torch.cat([first_bin_cent, bin_centers[1:-1], torch.tensor([bin_centers[-1]+right_bin_width/2.])]), \n",
    "                y=hists[:,1], \n",
    "                width = torch.cat([bin_centers[1]-first_bin_cent, bin_centers[2:]-bin_centers[1:-1],torch.tensor([right_bin_width])]),\n",
    "                marker=dict(\n",
    "                          opacity=0.3,\n",
    "                          color='orange',\n",
    "                          #size=4\n",
    "                      ),\n",
    "                name = 'Gain Corrected'\n",
    "            ),\n",
    "        ],\n",
    "        layout = plt_type.Layout(**layoutArgs)\n",
    "    )\n",
    "\n",
    "if exportNow:\n",
    "    exportFigure(fig, filename= 'ch1_figResultsCorrHistogram_MatchedMean_nf0'+ data_id + subdataset, \n",
    "                 time_stamp = time_stamp,\n",
    "                 type='plot', image='svg')\n",
    "else:\n",
    "    plt(fig)\n",
    "    \n",
    "if instant_clear_outputs:\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:preprocEnv]",
   "language": "python",
   "name": "conda-env-preprocEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
