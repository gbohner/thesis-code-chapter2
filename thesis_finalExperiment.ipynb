{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproc2P - final experiment\n",
    "\n",
    "This notebook sets up the final experiment, which is combining all covariance functions with all likelihoods and fitting them to all datasets\n",
    "\n",
    "## Datasets\n",
    "\n",
    "The datasets consist of the publicly available [Neurofinder datasets](http://neurofinder.codeneuro.org/)\n",
    "\n",
    "\n",
    "## The covariance functions\n",
    "\n",
    "These functions describe our prior assumptions about the spatial covariance of the illumination incident on the sample. The set of assumptions - going from less to more - are as follows:\n",
    "\n",
    "- No prior\n",
    "- Spatially smooth function (RBF covariance)\n",
    "- Smooth + Symmetric (XY or circular)\n",
    "- Smooth + Symmetric (XY or circular) + Anti-correlated at distance\n",
    "\n",
    "One can enforce the influence of the prior to a variable degree\n",
    "\n",
    "## Likelihood functions\n",
    "\n",
    "These functions describe the relationship between an observation and a corresponding sample of the Gaussian process output, and are as follows:\n",
    "\n",
    "- LinearGainLikelihood\n",
    "- PoissonInputPhotomultiplierLikelihood\n",
    "- PoissonInputUnderamplifiedPhotomultiplierLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocExperimentSetup import *\n",
    "# File system management\n",
    "import os\n",
    "import errno\n",
    "import zipfile\n",
    "\n",
    "# Get current git hash to ensure reproducible results\n",
    "import git\n",
    "git_cur_repo = git.Repo(search_parent_directories=True)\n",
    "git_cur_sha = git_cur_repo.head.object.hexsha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which datasets do we want to work with\n",
    "data_dir='/nfs/data/gergo/Neurofinder_update/'\n",
    "all_dataset_names = [\n",
    "#                      'neurofinder.00.00', \n",
    "#                      'neurofinder.00.01', \n",
    "#                      'neurofinder.00.00.test',\n",
    "#                      'neurofinder.00.01.test',\n",
    "#                      'neurofinder.01.00', \n",
    "#                      'neurofinder.01.00.test',\n",
    "#                      'neurofinder.01.01.test',                     \n",
    "                     'neurofinder.02.00', \n",
    "                     'neurofinder.02.01',\n",
    "                     'neurofinder.02.00.test',\n",
    "                     'neurofinder.02.01.test',\n",
    "#                      'neurofinder.03.00', \n",
    "#                      'neurofinder.03.00.test',\n",
    "#                      'neurofinder.04.00',\n",
    "#                      'neurofinder.04.01',\n",
    "#                      'neurofinder.04.00.test',\n",
    "#                      'neurofinder.04.01.test'\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing datasets\n",
    "\n",
    "Determine for each dataset the location of missing pixels (that are often the result of computational buffering inconsistencies during fluorescence microscopy, and are detectable as lines of \"0\" values), and create two datasets, one where the missing data is marked with NaN values, and another where the missing data is imputed via local convolution. of nearby non-missing pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in all_dataset_names:\n",
    "    \n",
    "    print(dataset_name)\n",
    "    \n",
    "    # Check if data has already been preprocessed\n",
    "    if not os.path.exists(data_dir+dataset_name+'/'):\n",
    "        with zipfile.ZipFile(data_dir+dataset_name + \".zip\",\"r\") as zip_ref:\n",
    "            zip_ref.extractall(path = data_dir)\n",
    "            print(\"Successfully extracted raw zip\")\n",
    "    \n",
    "    \n",
    "    imputeDataset(dataset_name, max_T=2000, data_dir=data_dir, \n",
    "                        stamp='', force_redo=True, device='cpu',\n",
    "                        returnNans = False)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Successfully found and imputed missing pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting background pixels training data\n",
    "\n",
    "In order to avoid signal-induced time variation, we use heuristics to attempt we get evenly distributed background pixel samples from throughout the field of view for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.16450653076171"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_name = 'neurofinder.02.00'\n",
    "# imgsImputed, imgsNans = imputeDataset(dataset_name=dataset_name, max_T=500, data_dir=data_dir, \n",
    "#                             stamp='', force_redo=False, device='cpu', returnNans=True)\n",
    "# float(torch.isnan(imgsNans.contiguous().view(-1,1)).sum())/float(imgsNans.numel())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_training_data = '_gitsha_' + git_cur_sha + '_rPC_1_origPMgain_useNans'\n",
    "\n",
    "for dataset_name in all_dataset_names:    \n",
    "    print(dataset_name)\n",
    "\n",
    "    # Create training data with given parameters\n",
    "    trainingData = extractTrainingData(dataset_name, \n",
    "                                       max_T=500, \n",
    "                                       data_dir=data_dir, \n",
    "                                       remove_PCs = 1, \n",
    "                                       normalize_Fano = False,\n",
    "                                       use_imputed_data = False,\n",
    "                                       stamp=stamp_training_data, \n",
    "                                       force_redo='trainingData_only',\n",
    "                                       device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_training_data = '_gitsha_' + git_cur_sha + '_rPC_0_origPMgain_useNans'\n",
    "\n",
    "for dataset_name in all_dataset_names:    \n",
    "    print(dataset_name)\n",
    "\n",
    "    # Create training data with given parameters\n",
    "    trainingData = extractTrainingData(dataset_name, \n",
    "                                       max_T=500, \n",
    "                                       data_dir=data_dir, \n",
    "                                       remove_PCs = None, \n",
    "                                       normalize_Fano = False,\n",
    "                                       use_imputed_data = False,\n",
    "                                       stamp=stamp_training_data, \n",
    "                                       force_redo='trainingData_only',\n",
    "                                       device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_training_data = '_gitsha_' + git_cur_sha + '_rPC_1_normPMgain_useNans'\n",
    "\n",
    "for dataset_name in all_dataset_names:    \n",
    "    print(dataset_name)\n",
    "\n",
    "    # Create training data with given parameters\n",
    "    trainingData = extractTrainingData(dataset_name, \n",
    "                                       max_T=500, \n",
    "                                       data_dir=data_dir, \n",
    "                                       remove_PCs = 1, \n",
    "                                       normalize_Fano = True,\n",
    "                                       use_imputed_data = False,\n",
    "                                       stamp=stamp_training_data, \n",
    "                                       force_redo='trainingData_only',\n",
    "                                       device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp_training_data = '_gitsha_' + git_cur_sha + '_rPC_0_normPMgain_useNans'\n",
    "\n",
    "for dataset_name in all_dataset_names:    \n",
    "    print(dataset_name)\n",
    "\n",
    "    # Create training data with given parameters\n",
    "    trainingData = extractTrainingData(dataset_name, \n",
    "                                       max_T=500, \n",
    "                                       data_dir=data_dir, \n",
    "                                       remove_PCs = None, \n",
    "                                       normalize_Fano = True,\n",
    "                                       use_imputed_data = False,\n",
    "                                       stamp=stamp_training_data, \n",
    "                                       force_redo='trainingData_only',\n",
    "                                       device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and run model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from examineTrainingData import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the appropriate training data sample\n",
    "stamp_git = '_gitsha_' + '2bd0d720de0995be6b0f1795304839f9877cb6c3'\n",
    "stamp_training_type = '_rPC_1_origPMgain_useNans'\n",
    "\n",
    "\n",
    "dataset_name = 'neurofinder.00.00'\n",
    "trainingData = loadTrainingData(\n",
    "        dataset_name = dataset_name,\n",
    "        data_dir = data_dir,\n",
    "        stamp = stamp_git + stamp_training_type\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the training data before use\n",
    "trainingDataUniform = downsampleTrainingData(trainingData, filter_width= 35, targetCoverage=0.01)\n",
    "stamp_trainingCoverage = '_targetCoverage_01' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import preprocExperimentSetup\n",
    "# importlib.reload(preprocExperimentSetup)\n",
    "# from preprocExperimentSetup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set up all combinations\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Priors\n",
    "all_priors = [\n",
    "{\n",
    "    'mean' : gpytorch.means.ZeroMean(),\n",
    "    'kernel' : preprocKernels.WhiteNoiseKernelBugfix(variances=torch.tensor([10.]))\n",
    "             },\n",
    "# {\n",
    "#     'mean' : gpytorch.means.ConstantMean(),\n",
    "#     'kernel' : k1\n",
    "# }\n",
    "]\n",
    "\n",
    "# Likelihoods\n",
    "all_likelihood_classes = [\n",
    "   #preprocLikelihoods.LinearGainLikelihood,\n",
    "   #preprocLikelihoods.PoissonInputPhotomultiplierLikelihood,\n",
    "   preprocLikelihoods.PoissonInputUnderamplifiedPhotomultiplierLikelihood\n",
    "]\n",
    "\n",
    "#cur_stamp = '_'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "\n",
    "\n",
    "for likelihood_class, prior_model_base in itertools.product(all_likelihood_classes, all_priors):\n",
    "    \n",
    "    print(dataset_name, likelihood_class, prior_model_base)\n",
    "    \n",
    "    \n",
    "    # Set up current prior and likelihood\n",
    "    prior_model = {\n",
    "        'mean' : copy.deepcopy(prior_model_base['mean']),\n",
    "        'kernel' : copy.deepcopy(prior_model_base['kernel'])\n",
    "    }\n",
    "    \n",
    "    likelihood_model = likelihood_class()\n",
    "    \n",
    "    # Clean cuda cache if being used\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run the training\n",
    "    stamp_modelGridType = '_grid_25_5'\n",
    "    mll=trainModel(dataset_name, trainingData, prior_model, likelihood_model, device='cpu',\n",
    "               data_dir='/nfs/data/gergo/Neurofinder_update/',\n",
    "               stamp = stamp_git + stamp_training_type + stamp_trainingCoverage + stamp_modelGridType,\n",
    "               n_iter = 30, x_batchsize=2**13, y_batchsize = 200, manual_seed=2713,\n",
    "               verbose = 2,\n",
    "               model_grid_size = 25,\n",
    "               model_interp_point_number = 5\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up all combinations\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Priors\n",
    "all_priors = [\n",
    "{\n",
    "    'mean' : gpytorch.means.ZeroMean(),\n",
    "    'kernel' : preprocKernels.WhiteNoiseKernelBugfix(variances=torch.tensor([10.]))\n",
    "             },\n",
    "# {\n",
    "#     'mean' : gpytorch.means.ConstantMean(),\n",
    "#     'kernel' : k1\n",
    "# }\n",
    "]\n",
    "\n",
    "# Likelihoods\n",
    "all_likelihood_classes = [\n",
    "   preprocLikelihoods.LinearGainLikelihood,\n",
    "   preprocLikelihoods.PoissonInputPhotomultiplierLikelihood,\n",
    "   preprocLikelihoods.PoissonInputUnderamplifiedPhotomultiplierLikelihood\n",
    "]\n",
    "\n",
    "#cur_stamp = '_'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "cur_stamp = git_cur_sha + '_02_normPMGain_test_05'\n",
    "\n",
    "for likelihood_class, prior_model_base, dataset_name in itertools.product(all_likelihood_classes, all_priors, all_dataset_names):\n",
    "    \n",
    "    print(dataset_name, likelihood_class, prior_model_base)\n",
    "    \n",
    "    # Create training data\n",
    "    trainingData = extractTrainingData(dataset_name, max_T=500, \n",
    "                                       data_dir='/nfs/data3/gergo/Neurofinder_update/', \n",
    "                                       remove_PCs = None, normalize_Fano = True,\n",
    "                                       stamp=cur_stamp, force_redo='trainingData_only')\n",
    "    \n",
    "    # Set up current prior and likelihood\n",
    "    prior_model = {\n",
    "        'mean' : copy.deepcopy(prior_model_base['mean']),\n",
    "        'kernel' : copy.deepcopy(prior_model_base['kernel'])\n",
    "    }\n",
    "    \n",
    "    likelihood_model = likelihood_class()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run the training\n",
    "    mll=trainModel(dataset_name, trainingData, prior_model, likelihood_model, device='cuda:2',\n",
    "               data_dir='/nfs/data3/gergo/Neurofinder_update/',\n",
    "               stamp = cur_stamp,\n",
    "               n_iter = 30, x_batchsize=2**13, y_batchsize = 200, manual_seed=2713,\n",
    "               verbose = 2)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up all combinations\n",
    "import itertools\n",
    "\n",
    "# Datasets\n",
    "all_dataset_names = ['neurofinder.00.00', 'neurofinder.01.00', 'neurofinder.02.00', 'neurofinder.03.00', 'neurofinder.04.00']\n",
    "\n",
    "# Priors\n",
    "all_priors = [\n",
    "{\n",
    "    'mean' : gpytorch.means.ZeroMean(),\n",
    "    'kernel' : preprocKernels.WhiteNoiseKernelBugfix(variances=torch.tensor([10.]))\n",
    "             },\n",
    "# {\n",
    "#     'mean' : gpytorch.means.ConstantMean(),\n",
    "#     'kernel' : k1\n",
    "# }\n",
    "]\n",
    "\n",
    "# Likelihoods\n",
    "all_likelihood_classes = [\n",
    "   #preprocLikelihoods.LinearGainLikelihood,\n",
    "   preprocLikelihoods.PoissonInputPhotomultiplierLikelihood,\n",
    "   #preprocLikelihoods.PoissonInputUnderamplifiedPhotomultiplierLikelihood\n",
    "]\n",
    "\n",
    "#cur_stamp = '_'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "cur_stamp = '_02_origPMGain_test_05'\n",
    "\n",
    "for likelihood_class, prior_model_base, dataset_name in itertools.product(all_likelihood_classes, all_priors, all_dataset_names):\n",
    "    \n",
    "    print(dataset_name, likelihood_class, prior_model_base)\n",
    "    \n",
    "    # Create training data\n",
    "    trainingData = extractTrainingData(dataset_name, max_T=500, \n",
    "                                       data_dir='/nfs/data3/gergo/Neurofinder_update/', \n",
    "                                       remove_PCs = None, normalize_Fano = False,\n",
    "                                       stamp=cur_stamp, force_redo='trainingData_only')\n",
    "    \n",
    "    # Set up current prior and likelihood\n",
    "    prior_model = {\n",
    "        'mean' : copy.deepcopy(prior_model_base['mean']),\n",
    "        'kernel' : copy.deepcopy(prior_model_base['kernel'])\n",
    "    }\n",
    "    \n",
    "    likelihood_model = likelihood_class()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run the training\n",
    "    mll=trainModel(dataset_name, trainingData, prior_model, likelihood_model, device='cuda:2',\n",
    "               data_dir='/nfs/data3/gergo/Neurofinder_update/',\n",
    "               stamp = cur_stamp,\n",
    "               n_iter = 30, x_batchsize=2**13, y_batchsize = 200, manual_seed=2713,\n",
    "               verbose = 2)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design complex covariance function as prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocExperimentSetup import *\n",
    "# File system management\n",
    "import os\n",
    "import errno\n",
    "import zipfile\n",
    "\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import gpytorch\n",
    "\n",
    "import preprocUtils\n",
    "import preprocRandomVariables\n",
    "import preprocLikelihoods\n",
    "import preprocModels\n",
    "import preprocKernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additive covariance\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Linearly symmetrised kernel\n",
    "\n",
    "a1=gpytorch.kernels.RBFKernel()\n",
    "a2=gpytorch.kernels.RBFKernel()\n",
    "# a1=preprocKernels.MexicanHatKernel()\n",
    "# a2=preprocKernels.MexicanHatKernel()\n",
    "a1.register_parameter('log_lengthscale', preprocUtils.toTorchParam(7.0),\n",
    "                                prior = gpytorch.priors.SmoothedBoxPrior(4.5, 8., sigma=0.1))\n",
    "del a1.active_dims\n",
    "a1.register_buffer('active_dims', torch.tensor([0], dtype=torch.long))\n",
    "a2.register_parameter('log_lengthscale', preprocUtils.toTorchParam(7.0),\n",
    "                                prior = gpytorch.priors.SmoothedBoxPrior(4.5, 8., sigma=0.1))\n",
    "del a2.active_dims\n",
    "a2.register_buffer('active_dims', torch.tensor([1], dtype=torch.long))\n",
    "\n",
    "linsymm_kernel_scanmirrors = preprocKernels.SymmetriseKernelLinearly(\n",
    "            a1+a2, \n",
    "            center=torch.tensor([0.5, 0.5]) \n",
    "        )\n",
    "linsymm_kernel_scanmirrors.center.requires_grad = False\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Radially symmetrised kernel\n",
    "\n",
    "b1 = preprocKernels.MexicanHatKernel()\n",
    "b1.register_parameter('log_lengthscale', preprocUtils.toTorchParam(6.0),\n",
    "                                prior = gpytorch.priors.SmoothedBoxPrior(4.5, 8., sigma=0.1))\n",
    "radsymm_kernel_objective = preprocKernels.SymmetriseKernelRadially(\n",
    "            b1, \n",
    "            center=torch.tensor([0.5, 0.5]) \n",
    "        )\n",
    "radsymm_kernel_objective.center.requires_grad = False\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Short lengthscale aberrations kernel\n",
    "\n",
    "base_kernel_aberrations = gpytorch.kernels.RBFKernel()\n",
    "base_kernel_aberrations.register_parameter('log_lengthscale', preprocUtils.toTorchParam(2.0),\n",
    "                                prior = gpytorch.priors.SmoothedBoxPrior(1., 3.5, sigma=0.1))\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ---------------------------------------------------------\n",
    "# Creating the additive kernel function\n",
    "\n",
    "expert_covariance_function = preprocKernels.ScaleKernel(\n",
    "    (\n",
    "        preprocKernels.ScaleKernel(\n",
    "            (\n",
    "                preprocKernels.ScaleKernel(\n",
    "                    linsymm_kernel_scanmirrors,\n",
    "                    1., fix_scale=False     \n",
    "                ) + (\n",
    "\n",
    "                preprocKernels.ScaleKernel(\n",
    "                    radsymm_kernel_objective,\n",
    "                    1., fix_scale=False      \n",
    "                )) \n",
    "            ),\n",
    "            1./math.exp(1.), fix_scale = True\n",
    "            \n",
    "        ) + (\n",
    "            \n",
    "            \n",
    "        preprocKernels.ScaleKernel(\n",
    "                base_kernel_aberrations,\n",
    "                1., fix_scale=True      \n",
    "        )\n",
    "        )\n",
    "    ),\n",
    "    1./math.exp(1.), fix_scale=False\n",
    ")\n",
    "\n",
    "\n",
    "str(expert_covariance_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up all combinations\n",
    "import itertools\n",
    "\n",
    "# Datasets\n",
    "all_dataset_names = ['neurofinder.00.00', 'neurofinder.01.00', 'neurofinder.02.00', 'neurofinder.03.00', 'neurofinder.04.00']\n",
    "\n",
    "# Priors\n",
    "all_priors = [\n",
    "{\n",
    "    'mean' : gpytorch.means.ConstantMean(),\n",
    "    'kernel' : expert_covariance_function\n",
    "}\n",
    "]\n",
    "\n",
    "# Likelihoods\n",
    "all_likelihood_classes = [\n",
    "   preprocLikelihoods.LinearGainLikelihood,\n",
    "   preprocLikelihoods.PoissonInputPhotomultiplierLikelihood,\n",
    "   #preprocLikelihoods.PoissonInputUnderamplifiedPhotomultiplierLikelihood\n",
    "]\n",
    "\n",
    "#cur_stamp = '_'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "cur_stamp = '_05_origPMGain_test_05_finegrid_50_3interp'\n",
    "\n",
    "for likelihood_class, prior_model_base, dataset_name in itertools.product(all_likelihood_classes, all_priors, all_dataset_names):\n",
    "    \n",
    "    print(dataset_name, likelihood_class, prior_model_base)\n",
    "    \n",
    "    # Create training data\n",
    "    trainingData = extractTrainingData(dataset_name, max_T=500, \n",
    "                                       data_dir='/nfs/data3/gergo/Neurofinder_update/', \n",
    "                                       remove_PCs = None, normalize_Fano = False,\n",
    "                                       stamp=cur_stamp, force_redo='trainingData_only')\n",
    "    \n",
    "    # Set up current prior and likelihood\n",
    "    prior_model = {\n",
    "        'mean' : copy.deepcopy(prior_model_base['mean']),\n",
    "        'kernel' : copy.deepcopy(prior_model_base['kernel'])\n",
    "    }\n",
    "    \n",
    "    likelihood_model = likelihood_class()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run the training\n",
    "    mll=trainModel(dataset_name, trainingData, prior_model, likelihood_model, device='cuda:2',\n",
    "               data_dir='/nfs/data3/gergo/Neurofinder_update/',\n",
    "               stamp = cur_stamp,\n",
    "               n_iter = 30, x_batchsize=2**13, y_batchsize = 200, manual_seed=2713,\n",
    "               verbose = 1)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up all combinations\n",
    "import itertools\n",
    "\n",
    "# Datasets\n",
    "all_dataset_names = ['neurofinder.00.00', 'neurofinder.01.00', 'neurofinder.02.00', 'neurofinder.03.00', 'neurofinder.04.00']\n",
    "\n",
    "# Priors\n",
    "all_priors = [\n",
    "{\n",
    "    'mean' : gpytorch.means.ConstantMean(),\n",
    "    'kernel' : expert_covariance_function\n",
    "}\n",
    "]\n",
    "\n",
    "# Likelihoods\n",
    "all_likelihood_classes = [\n",
    "   #preprocLikelihoods.LinearGainLikelihood,\n",
    "   preprocLikelihoods.PoissonInputPhotomultiplierLikelihood,\n",
    "   #preprocLikelihoods.PoissonInputUnderamplifiedPhotomultiplierLikelihood\n",
    "]\n",
    "\n",
    "#cur_stamp = '_'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "cur_stamp = '_02_normPMGain_test_05'\n",
    "\n",
    "for likelihood_class, prior_model_base, dataset_name in itertools.product(all_likelihood_classes, all_priors, all_dataset_names):\n",
    "    \n",
    "    print(dataset_name, likelihood_class, prior_model_base)\n",
    "    \n",
    "    # Create training data\n",
    "    trainingData = extractTrainingData(dataset_name, max_T=500, \n",
    "                                       data_dir='/nfs/data3/gergo/Neurofinder_update/', \n",
    "                                       remove_PCs = None, normalize_Fano = True,\n",
    "                                       stamp=cur_stamp, force_redo='trainingData_only')\n",
    "    \n",
    "    # Set up current prior and likelihood\n",
    "    prior_model = {\n",
    "        'mean' : copy.deepcopy(prior_model_base['mean']),\n",
    "        'kernel' : copy.deepcopy(prior_model_base['kernel'])\n",
    "    }\n",
    "    \n",
    "    likelihood_model = likelihood_class()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run the training\n",
    "    mll=trainModel(dataset_name, trainingData, prior_model, likelihood_model, device='cuda:1',\n",
    "               data_dir='/nfs/data3/gergo/Neurofinder_update/',\n",
    "               stamp = cur_stamp,\n",
    "               n_iter = 30, x_batchsize=2**13, y_batchsize = 200, manual_seed=2713,\n",
    "               verbose = 1)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up all combinations\n",
    "import itertools\n",
    "\n",
    "# Datasets\n",
    "all_dataset_names = ['neurofinder.00.00', 'neurofinder.01.00', 'neurofinder.02.00', 'neurofinder.03.00', 'neurofinder.04.00']\n",
    "\n",
    "# Priors\n",
    "all_priors = [\n",
    "{\n",
    "    'mean' : gpytorch.means.ConstantMean(),\n",
    "    'kernel' : expert_covariance_function\n",
    "}\n",
    "]\n",
    "\n",
    "# Likelihoods\n",
    "all_likelihood_classes = [\n",
    "   #preprocLikelihoods.LinearGainLikelihood,\n",
    "   #preprocLikelihoods.PoissonInputPhotomultiplierLikelihood,\n",
    "   preprocLikelihoods.PoissonInputUnderamplifiedPhotomultiplierLikelihood\n",
    "]\n",
    "\n",
    "#cur_stamp = '_'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "cur_stamp = '_02_origPMGain_test_05'\n",
    "\n",
    "for likelihood_class, prior_model_base, dataset_name in itertools.product(all_likelihood_classes, all_priors, all_dataset_names):\n",
    "    \n",
    "    print(dataset_name, likelihood_class, prior_model_base)\n",
    "    \n",
    "    # Create training data\n",
    "    trainingData = extractTrainingData(dataset_name, max_T=500, \n",
    "                                       data_dir='/nfs/data3/gergo/Neurofinder_update/', \n",
    "                                       remove_PCs = None, normalize_Fano = False,\n",
    "                                       stamp=cur_stamp, force_redo='trainingData_only')\n",
    "    \n",
    "    # Set up current prior and likelihood\n",
    "    prior_model = {\n",
    "        'mean' : copy.deepcopy(prior_model_base['mean']),\n",
    "        'kernel' : copy.deepcopy(prior_model_base['kernel'])\n",
    "    }\n",
    "    \n",
    "    likelihood_model = likelihood_class()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run the training\n",
    "    mll=trainModel(dataset_name, trainingData, prior_model, likelihood_model, device='cuda:1',\n",
    "               data_dir='/nfs/data3/gergo/Neurofinder_update/',\n",
    "               stamp = cur_stamp,\n",
    "               n_iter = 30, x_batchsize=2**13, y_batchsize = 200, manual_seed=2713,\n",
    "               verbose = 1)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up all combinations\n",
    "import itertools\n",
    "\n",
    "# Datasets\n",
    "all_dataset_names = ['neurofinder.00.00', 'neurofinder.01.00', 'neurofinder.02.00', 'neurofinder.03.00', 'neurofinder.04.00']\n",
    "\n",
    "# Priors\n",
    "all_priors = [\n",
    "{\n",
    "    'mean' : gpytorch.means.ConstantMean(),\n",
    "    'kernel' : expert_covariance_function\n",
    "}\n",
    "]\n",
    "\n",
    "# Likelihoods\n",
    "all_likelihood_classes = [\n",
    "   #preprocLikelihoods.LinearGainLikelihood,\n",
    "   #preprocLikelihoods.PoissonInputPhotomultiplierLikelihood,\n",
    "   preprocLikelihoods.PoissonInputUnderamplifiedPhotomultiplierLikelihood\n",
    "]\n",
    "\n",
    "#cur_stamp = '_'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "cur_stamp = '_02_normPMGain_test_05'\n",
    "\n",
    "for likelihood_class, prior_model_base, dataset_name in itertools.product(all_likelihood_classes, all_priors, all_dataset_names):\n",
    "    \n",
    "    print(dataset_name, likelihood_class, prior_model_base)\n",
    "    \n",
    "    # Create training data\n",
    "    trainingData = extractTrainingData(dataset_name, max_T=500, \n",
    "                                       data_dir='/nfs/data3/gergo/Neurofinder_update/', \n",
    "                                       remove_PCs = None, normalize_Fano = True,\n",
    "                                       stamp=cur_stamp, force_redo='trainingData_only')\n",
    "    \n",
    "    # Set up current prior and likelihood\n",
    "    prior_model = {\n",
    "        'mean' : copy.deepcopy(prior_model_base['mean']),\n",
    "        'kernel' : copy.deepcopy(prior_model_base['kernel'])\n",
    "    }\n",
    "    \n",
    "    likelihood_model = likelihood_class()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run the training\n",
    "    mll=trainModel(dataset_name, trainingData, prior_model, likelihood_model, device='cuda:1',\n",
    "               data_dir='/nfs/data3/gergo/Neurofinder_update/',\n",
    "               stamp = cur_stamp,\n",
    "               n_iter = 30, x_batchsize=2**13, y_batchsize = 200, manual_seed=2713,\n",
    "               verbose = 1)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:preprocEnv]",
   "language": "python",
   "name": "conda-env-preprocEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
